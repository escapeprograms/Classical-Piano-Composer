{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run with Python 3.11\n",
    "import glob\n",
    "import pickle\n",
    "import numpy\n",
    "from music21 import converter, instrument, note, chord\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparams\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes(load_existing = True):\n",
    "    \"\"\" Get all the notes and chords from the midi files in the ./midi_songs directory \"\"\"\n",
    "    if load_existing:\n",
    "        print(\"loading existing notes\")\n",
    "        with open('data/notes', 'rb') as filepath:\n",
    "            notes = pickle.load(filepath)\n",
    "            return notes\n",
    "    \n",
    "    notes = []\n",
    "\n",
    "    for file in glob.glob(\"midi_songs/*.mid\"):\n",
    "        midi = converter.parse(file)\n",
    "\n",
    "        print(\"Parsing %s\" % file)\n",
    "\n",
    "        notes_to_parse = None\n",
    "\n",
    "        try: # file has instrument parts\n",
    "            s2 = instrument.partitionByInstrument(midi)\n",
    "            notes_to_parse = s2.parts[0].recurse() \n",
    "        except: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "\n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    with open('data/notes', 'wb') as filepath:\n",
    "        pickle.dump(notes, filepath)\n",
    "\n",
    "    return notes\n",
    "\n",
    "\n",
    "#assemble training data in a readable format \n",
    "def prepare_sequences(notes, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    sequence_length = 100\n",
    "\n",
    "    # get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "     # create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    network_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    # normalize input\n",
    "    network_input = network_input / float(n_vocab)\n",
    "\n",
    "    network_output = F.one_hot(torch.tensor(network_output)) #one hot encoding\n",
    "\n",
    "    return (network_input, network_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing midi_songs\\0fithos.mid\n",
      "Parsing midi_songs\\8.mid\n",
      "Parsing midi_songs\\ahead_on_our_way_piano.mid\n",
      "Parsing midi_songs\\AT.mid\n",
      "Parsing midi_songs\\balamb.mid\n",
      "Parsing midi_songs\\bcm.mid\n",
      "Parsing midi_songs\\BlueStone_LastDungeon.mid\n",
      "Parsing midi_songs\\braska.mid\n",
      "Parsing midi_songs\\caitsith.mid\n",
      "Parsing midi_songs\\Cids.mid\n",
      "Parsing midi_songs\\cosmo.mid\n",
      "Parsing midi_songs\\costadsol.mid\n",
      "Parsing midi_songs\\dayafter.mid\n",
      "Parsing midi_songs\\decisive.mid\n",
      "Parsing midi_songs\\dontbeafraid.mid\n",
      "Parsing midi_songs\\DOS.mid\n",
      "Parsing midi_songs\\electric_de_chocobo.mid\n",
      "Parsing midi_songs\\Eternal_Harvest.mid\n",
      "Parsing midi_songs\\EyesOnMePiano.mid\n",
      "Parsing midi_songs\\ff11_awakening_piano.mid\n",
      "Parsing midi_songs\\ff1battp.mid\n",
      "Parsing midi_songs\\FF3_Battle_(Piano).mid\n",
      "Parsing midi_songs\\FF3_Third_Phase_Final_(Piano).mid\n",
      "Parsing midi_songs\\ff4-airship.mid\n",
      "Parsing midi_songs\\Ff4-BattleLust.mid\n",
      "Parsing midi_songs\\ff4-fight1.mid\n",
      "Parsing midi_songs\\ff4-town.mid\n",
      "Parsing midi_songs\\FF4.mid\n",
      "Parsing midi_songs\\ff4pclov.mid\n",
      "Parsing midi_songs\\ff4_piano_collections-main_theme.mid\n",
      "Parsing midi_songs\\FF6epitaph_piano.mid\n",
      "Parsing midi_songs\\ff6shap.mid\n",
      "Parsing midi_songs\\Ff7-Cinco.mid\n",
      "Parsing midi_songs\\Ff7-Jenova_Absolute.mid\n",
      "Parsing midi_songs\\ff7-mainmidi.mid\n",
      "Parsing midi_songs\\Ff7-One_Winged.mid\n",
      "Parsing midi_songs\\ff7themep.mid\n",
      "Parsing midi_songs\\ff8-lfp.mid\n",
      "Parsing midi_songs\\FF8_Shuffle_or_boogie_pc.mid\n",
      "Parsing midi_songs\\FFIII_Edgar_And_Sabin_Piano.mid\n",
      "Parsing midi_songs\\FFIXQuMarshP.mid\n",
      "Parsing midi_songs\\FFIX_Piano.mid\n",
      "Parsing midi_songs\\FFVII_BATTLE.mid\n",
      "Parsing midi_songs\\FFX_-_Ending_Theme_(Piano_Version)_-_by_Angel_FF.mid\n",
      "Parsing midi_songs\\Fiend_Battle_(Piano).mid\n",
      "Parsing midi_songs\\Fierce_Battle_(Piano).mid\n",
      "Parsing midi_songs\\figaro.mid\n",
      "Parsing midi_songs\\Finalfantasy5gilgameshp.mid\n",
      "Parsing midi_songs\\Finalfantasy6fanfarecomplete.mid\n",
      "Parsing midi_songs\\Final_Fantasy_7_-_Judgement_Day_Piano.mid\n",
      "Parsing midi_songs\\Final_Fantasy_Matouyas_Cave_Piano.mid\n",
      "Parsing midi_songs\\fortresscondor.mid\n",
      "Parsing midi_songs\\Fyw_piano.mid\n",
      "Parsing midi_songs\\gerudo.mid\n",
      "Parsing midi_songs\\goldsaucer.mid\n",
      "Parsing midi_songs\\Gold_Silver_Rival_Battle.mid\n",
      "Parsing midi_songs\\great_war.mid\n",
      "Parsing midi_songs\\HighwindTakestotheSkies.mid\n",
      "Parsing midi_songs\\In_Zanarkand.mid\n",
      "Parsing midi_songs\\JENOVA.mid\n",
      "Parsing midi_songs\\Kingdom_Hearts_Dearly_Beloved.mid\n",
      "Parsing midi_songs\\Kingdom_Hearts_Traverse_Town.mid\n",
      "Parsing midi_songs\\Life_Stream.mid\n",
      "Parsing midi_songs\\lurk_in_dark.mid\n",
      "Parsing midi_songs\\mining.mid\n",
      "Parsing midi_songs\\Oppressed.mid\n",
      "Parsing midi_songs\\OTD5YA.mid\n",
      "Parsing midi_songs\\path_of_repentance.mid\n",
      "Parsing midi_songs\\pkelite4.mid\n",
      "Parsing midi_songs\\Rachel_Piano_tempofix.mid\n",
      "Parsing midi_songs\\redwings.mid\n",
      "Parsing midi_songs\\relmstheme-piano.mid\n",
      "Parsing midi_songs\\roseofmay-piano.mid\n",
      "Parsing midi_songs\\rufus.mid\n",
      "Parsing midi_songs\\Rydia_pc.mid\n",
      "Parsing midi_songs\\sandy.mid\n",
      "Parsing midi_songs\\sera_.mid\n",
      "Parsing midi_songs\\sobf.mid\n",
      "Parsing midi_songs\\Still_Alive-1.mid\n",
      "Parsing midi_songs\\Suteki_Da_Ne_(Piano_Version).mid\n",
      "Parsing midi_songs\\thenightmarebegins.mid\n",
      "Parsing midi_songs\\thoughts.mid\n",
      "Parsing midi_songs\\tifap.mid\n",
      "Parsing midi_songs\\tpirtsd-piano.mid\n",
      "Parsing midi_songs\\traitor.mid\n",
      "Parsing midi_songs\\ultimafro.mid\n",
      "Parsing midi_songs\\ultros.mid\n",
      "Parsing midi_songs\\VincentPiano.mid\n",
      "Parsing midi_songs\\ViviinAlexandria.mid\n",
      "Parsing midi_songs\\waltz_de_choco.mid\n",
      "Parsing midi_songs\\Zelda_Overworld.mid\n",
      "Parsing midi_songs\\z_aeristhemepiano.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Archimedes Li\\AppData\\Local\\Temp\\ipykernel_26124\\647225724.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output_tensor = torch.tensor(network_output, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "notes = get_notes(load_existing=False) # set load_existing to True to skip reading in midi\n",
    "\n",
    "# get amount of pitch names\n",
    "n_vocab = len(set(notes))\n",
    "\n",
    "network_input, network_output = prepare_sequences(notes, n_vocab)\n",
    "\n",
    "#build the dataset + dataloader\n",
    "input_tensor = torch.tensor(network_input, dtype=torch.float32)\n",
    "output_tensor = torch.tensor(network_output, dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(input_tensor, output_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import MusicLSTM\n",
    "\n",
    "def train(num_epochs, model, train_dataloader, loss_func, optimizer):\n",
    "    losses = []\n",
    "    total_steps = len(train_dataloader)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch, (sequence, next_note) in enumerate(train_dataloader):\n",
    "    \n",
    "            output = model(sequence)\n",
    "            loss = loss_func(output, next_note)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (batch+1)%100 == 0:\n",
    "                print(f\"Epoch: {epoch+1}; Batch {batch+1} / {total_steps}; Loss: {loss.item():>4f}\")\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1; Batch 100 / 359; Loss: 4.788625\n",
      "Epoch: 1; Batch 200 / 359; Loss: 4.526455\n",
      "Epoch: 1; Batch 300 / 359; Loss: 4.678929\n",
      "Epoch: 2; Batch 100 / 359; Loss: 4.586399\n",
      "Epoch: 2; Batch 200 / 359; Loss: 4.725073\n",
      "Epoch: 2; Batch 300 / 359; Loss: 4.696541\n",
      "Epoch: 3; Batch 100 / 359; Loss: 4.561501\n",
      "Epoch: 3; Batch 200 / 359; Loss: 4.687592\n",
      "Epoch: 3; Batch 300 / 359; Loss: 4.735240\n",
      "Epoch: 4; Batch 100 / 359; Loss: 4.706417\n",
      "Epoch: 4; Batch 200 / 359; Loss: 4.801732\n",
      "Epoch: 4; Batch 300 / 359; Loss: 4.713769\n",
      "Epoch: 5; Batch 100 / 359; Loss: 4.754982\n",
      "Epoch: 5; Batch 200 / 359; Loss: 4.700130\n",
      "Epoch: 5; Batch 300 / 359; Loss: 4.628604\n",
      "Epoch: 6; Batch 100 / 359; Loss: 4.580406\n",
      "Epoch: 6; Batch 200 / 359; Loss: 4.611865\n",
      "Epoch: 6; Batch 300 / 359; Loss: 4.390574\n",
      "Epoch: 7; Batch 100 / 359; Loss: 4.468637\n",
      "Epoch: 7; Batch 200 / 359; Loss: 4.413692\n",
      "Epoch: 7; Batch 300 / 359; Loss: 4.546490\n",
      "Epoch: 8; Batch 100 / 359; Loss: 4.305113\n",
      "Epoch: 8; Batch 200 / 359; Loss: 4.416423\n",
      "Epoch: 8; Batch 300 / 359; Loss: 4.349290\n",
      "Epoch: 9; Batch 100 / 359; Loss: 4.329375\n",
      "Epoch: 9; Batch 200 / 359; Loss: 4.254702\n",
      "Epoch: 9; Batch 300 / 359; Loss: 4.092321\n",
      "Epoch: 10; Batch 100 / 359; Loss: 4.299128\n",
      "Epoch: 10; Batch 200 / 359; Loss: 4.187183\n",
      "Epoch: 10; Batch 300 / 359; Loss: 4.509383\n",
      "Epoch: 11; Batch 100 / 359; Loss: 4.217336\n",
      "Epoch: 11; Batch 200 / 359; Loss: 4.023446\n",
      "Epoch: 11; Batch 300 / 359; Loss: 4.405364\n",
      "Epoch: 12; Batch 100 / 359; Loss: 4.241219\n",
      "Epoch: 12; Batch 200 / 359; Loss: 4.271674\n",
      "Epoch: 12; Batch 300 / 359; Loss: 4.329129\n",
      "Epoch: 13; Batch 100 / 359; Loss: 4.282697\n",
      "Epoch: 13; Batch 200 / 359; Loss: 4.344680\n",
      "Epoch: 13; Batch 300 / 359; Loss: 4.338457\n",
      "Epoch: 14; Batch 100 / 359; Loss: 4.138036\n",
      "Epoch: 14; Batch 200 / 359; Loss: 4.106512\n",
      "Epoch: 14; Batch 300 / 359; Loss: 4.168854\n",
      "Epoch: 15; Batch 100 / 359; Loss: 4.253479\n",
      "Epoch: 15; Batch 200 / 359; Loss: 4.097691\n",
      "Epoch: 15; Batch 300 / 359; Loss: 4.219201\n",
      "Epoch: 16; Batch 100 / 359; Loss: 4.258220\n",
      "Epoch: 16; Batch 200 / 359; Loss: 4.164271\n",
      "Epoch: 16; Batch 300 / 359; Loss: 4.003794\n",
      "Epoch: 17; Batch 100 / 359; Loss: 3.945185\n",
      "Epoch: 17; Batch 200 / 359; Loss: 3.939190\n",
      "Epoch: 17; Batch 300 / 359; Loss: 3.964295\n",
      "Epoch: 18; Batch 100 / 359; Loss: 3.892435\n",
      "Epoch: 18; Batch 200 / 359; Loss: 3.832975\n",
      "Epoch: 18; Batch 300 / 359; Loss: 4.050840\n",
      "Epoch: 19; Batch 100 / 359; Loss: 3.815470\n",
      "Epoch: 19; Batch 200 / 359; Loss: 4.114417\n",
      "Epoch: 19; Batch 300 / 359; Loss: 4.009905\n",
      "Epoch: 20; Batch 100 / 359; Loss: 3.865800\n",
      "Epoch: 20; Batch 200 / 359; Loss: 3.956058\n",
      "Epoch: 20; Batch 300 / 359; Loss: 3.873005\n",
      "Epoch: 21; Batch 100 / 359; Loss: 3.738435\n",
      "Epoch: 21; Batch 200 / 359; Loss: 3.871921\n",
      "Epoch: 21; Batch 300 / 359; Loss: 3.799204\n",
      "Epoch: 22; Batch 100 / 359; Loss: 3.681445\n",
      "Epoch: 22; Batch 200 / 359; Loss: 3.718349\n",
      "Epoch: 22; Batch 300 / 359; Loss: 3.814045\n",
      "Epoch: 23; Batch 100 / 359; Loss: 3.501736\n",
      "Epoch: 23; Batch 200 / 359; Loss: 3.724541\n",
      "Epoch: 23; Batch 300 / 359; Loss: 3.918192\n",
      "Epoch: 24; Batch 100 / 359; Loss: 3.506576\n",
      "Epoch: 24; Batch 200 / 359; Loss: 3.901034\n",
      "Epoch: 24; Batch 300 / 359; Loss: 3.959322\n",
      "Epoch: 25; Batch 100 / 359; Loss: 3.650115\n",
      "Epoch: 25; Batch 200 / 359; Loss: 4.051187\n",
      "Epoch: 25; Batch 300 / 359; Loss: 3.872060\n",
      "Epoch: 26; Batch 100 / 359; Loss: 3.559232\n",
      "Epoch: 26; Batch 200 / 359; Loss: 3.739062\n",
      "Epoch: 26; Batch 300 / 359; Loss: 3.354282\n",
      "Epoch: 27; Batch 100 / 359; Loss: 3.714699\n",
      "Epoch: 27; Batch 200 / 359; Loss: 3.785260\n",
      "Epoch: 27; Batch 300 / 359; Loss: 3.721177\n",
      "Epoch: 28; Batch 100 / 359; Loss: 3.381745\n",
      "Epoch: 28; Batch 200 / 359; Loss: 3.552339\n",
      "Epoch: 28; Batch 300 / 359; Loss: 3.686758\n",
      "Epoch: 29; Batch 100 / 359; Loss: 3.640597\n",
      "Epoch: 29; Batch 200 / 359; Loss: 3.588243\n",
      "Epoch: 29; Batch 300 / 359; Loss: 3.410213\n",
      "Epoch: 30; Batch 100 / 359; Loss: 3.524312\n",
      "Epoch: 30; Batch 200 / 359; Loss: 3.433969\n",
      "Epoch: 30; Batch 300 / 359; Loss: 3.570007\n",
      "Epoch: 31; Batch 100 / 359; Loss: 3.499156\n",
      "Epoch: 31; Batch 200 / 359; Loss: 3.338207\n",
      "Epoch: 31; Batch 300 / 359; Loss: 3.374938\n",
      "Epoch: 32; Batch 100 / 359; Loss: 3.598399\n",
      "Epoch: 32; Batch 200 / 359; Loss: 3.368139\n",
      "Epoch: 32; Batch 300 / 359; Loss: 3.346838\n",
      "Epoch: 33; Batch 100 / 359; Loss: 3.231085\n",
      "Epoch: 33; Batch 200 / 359; Loss: 3.501898\n",
      "Epoch: 33; Batch 300 / 359; Loss: 3.361144\n",
      "Epoch: 34; Batch 100 / 359; Loss: 3.302676\n",
      "Epoch: 34; Batch 200 / 359; Loss: 3.283609\n",
      "Epoch: 34; Batch 300 / 359; Loss: 3.386468\n",
      "Epoch: 35; Batch 100 / 359; Loss: 3.426839\n",
      "Epoch: 35; Batch 200 / 359; Loss: 3.249976\n",
      "Epoch: 35; Batch 300 / 359; Loss: 3.165505\n",
      "Epoch: 36; Batch 100 / 359; Loss: 3.237322\n",
      "Epoch: 36; Batch 200 / 359; Loss: 3.406608\n",
      "Epoch: 36; Batch 300 / 359; Loss: 3.380108\n",
      "Epoch: 37; Batch 100 / 359; Loss: 3.247622\n",
      "Epoch: 37; Batch 200 / 359; Loss: 3.035465\n",
      "Epoch: 37; Batch 300 / 359; Loss: 3.367684\n",
      "Epoch: 38; Batch 100 / 359; Loss: 3.024958\n",
      "Epoch: 38; Batch 200 / 359; Loss: 3.230637\n",
      "Epoch: 38; Batch 300 / 359; Loss: 3.182189\n",
      "Epoch: 39; Batch 100 / 359; Loss: 2.995136\n",
      "Epoch: 39; Batch 200 / 359; Loss: 3.339844\n",
      "Epoch: 39; Batch 300 / 359; Loss: 3.167397\n",
      "Epoch: 40; Batch 100 / 359; Loss: 3.040316\n",
      "Epoch: 40; Batch 200 / 359; Loss: 3.098922\n",
      "Epoch: 40; Batch 300 / 359; Loss: 2.972214\n",
      "Epoch: 41; Batch 100 / 359; Loss: 3.119820\n",
      "Epoch: 41; Batch 200 / 359; Loss: 3.141195\n",
      "Epoch: 41; Batch 300 / 359; Loss: 3.094975\n",
      "Epoch: 42; Batch 100 / 359; Loss: 2.947005\n",
      "Epoch: 42; Batch 200 / 359; Loss: 3.028723\n",
      "Epoch: 42; Batch 300 / 359; Loss: 2.948713\n",
      "Epoch: 43; Batch 100 / 359; Loss: 2.860239\n",
      "Epoch: 43; Batch 200 / 359; Loss: 3.115289\n",
      "Epoch: 43; Batch 300 / 359; Loss: 3.076934\n",
      "Epoch: 44; Batch 100 / 359; Loss: 3.054225\n",
      "Epoch: 44; Batch 200 / 359; Loss: 2.985022\n",
      "Epoch: 44; Batch 300 / 359; Loss: 3.029347\n",
      "Epoch: 45; Batch 100 / 359; Loss: 2.819880\n",
      "Epoch: 45; Batch 200 / 359; Loss: 2.933126\n",
      "Epoch: 45; Batch 300 / 359; Loss: 3.426097\n",
      "Epoch: 46; Batch 100 / 359; Loss: 2.903123\n",
      "Epoch: 46; Batch 200 / 359; Loss: 2.814155\n",
      "Epoch: 46; Batch 300 / 359; Loss: 3.084746\n",
      "Epoch: 47; Batch 100 / 359; Loss: 2.928722\n",
      "Epoch: 47; Batch 200 / 359; Loss: 3.007951\n",
      "Epoch: 47; Batch 300 / 359; Loss: 2.989411\n",
      "Epoch: 48; Batch 100 / 359; Loss: 2.910390\n",
      "Epoch: 48; Batch 200 / 359; Loss: 2.792681\n",
      "Epoch: 48; Batch 300 / 359; Loss: 2.820064\n",
      "Epoch: 49; Batch 100 / 359; Loss: 2.751317\n",
      "Epoch: 49; Batch 200 / 359; Loss: 2.849278\n",
      "Epoch: 49; Batch 300 / 359; Loss: 2.963991\n",
      "Epoch: 50; Batch 100 / 359; Loss: 2.700680\n",
      "Epoch: 50; Batch 200 / 359; Loss: 2.593919\n",
      "Epoch: 50; Batch 300 / 359; Loss: 2.971810\n",
      "Epoch: 51; Batch 100 / 359; Loss: 2.798049\n",
      "Epoch: 51; Batch 200 / 359; Loss: 2.923818\n",
      "Epoch: 51; Batch 300 / 359; Loss: 2.761855\n",
      "Epoch: 52; Batch 100 / 359; Loss: 2.890825\n",
      "Epoch: 52; Batch 200 / 359; Loss: 2.699027\n",
      "Epoch: 52; Batch 300 / 359; Loss: 2.849771\n",
      "Epoch: 53; Batch 100 / 359; Loss: 2.798434\n",
      "Epoch: 53; Batch 200 / 359; Loss: 2.726443\n",
      "Epoch: 53; Batch 300 / 359; Loss: 2.761892\n",
      "Epoch: 54; Batch 100 / 359; Loss: 2.766279\n",
      "Epoch: 54; Batch 200 / 359; Loss: 2.993719\n",
      "Epoch: 54; Batch 300 / 359; Loss: 2.736210\n",
      "Epoch: 55; Batch 100 / 359; Loss: 2.609934\n",
      "Epoch: 55; Batch 200 / 359; Loss: 2.787479\n",
      "Epoch: 55; Batch 300 / 359; Loss: 2.768416\n",
      "Epoch: 56; Batch 100 / 359; Loss: 2.720101\n",
      "Epoch: 56; Batch 200 / 359; Loss: 2.693736\n",
      "Epoch: 56; Batch 300 / 359; Loss: 2.690277\n",
      "Epoch: 57; Batch 100 / 359; Loss: 2.610981\n",
      "Epoch: 57; Batch 200 / 359; Loss: 2.601674\n",
      "Epoch: 57; Batch 300 / 359; Loss: 2.790312\n",
      "Epoch: 58; Batch 100 / 359; Loss: 2.658947\n",
      "Epoch: 58; Batch 200 / 359; Loss: 2.571781\n",
      "Epoch: 58; Batch 300 / 359; Loss: 2.730405\n",
      "Epoch: 59; Batch 100 / 359; Loss: 2.774700\n",
      "Epoch: 59; Batch 200 / 359; Loss: 2.810727\n",
      "Epoch: 59; Batch 300 / 359; Loss: 2.584394\n",
      "Epoch: 60; Batch 100 / 359; Loss: 2.754179\n",
      "Epoch: 60; Batch 200 / 359; Loss: 2.695358\n",
      "Epoch: 60; Batch 300 / 359; Loss: 2.636803\n",
      "Epoch: 61; Batch 100 / 359; Loss: 2.567196\n",
      "Epoch: 61; Batch 200 / 359; Loss: 2.710305\n",
      "Epoch: 61; Batch 300 / 359; Loss: 2.991063\n",
      "Epoch: 62; Batch 100 / 359; Loss: 2.488245\n",
      "Epoch: 62; Batch 200 / 359; Loss: 2.684698\n",
      "Epoch: 62; Batch 300 / 359; Loss: 2.505848\n",
      "Epoch: 63; Batch 100 / 359; Loss: 2.486331\n",
      "Epoch: 63; Batch 200 / 359; Loss: 2.359674\n",
      "Epoch: 63; Batch 300 / 359; Loss: 2.550890\n",
      "Epoch: 64; Batch 100 / 359; Loss: 2.669847\n",
      "Epoch: 64; Batch 200 / 359; Loss: 2.506102\n",
      "Epoch: 64; Batch 300 / 359; Loss: 2.449930\n",
      "Epoch: 65; Batch 100 / 359; Loss: 2.571728\n",
      "Epoch: 65; Batch 200 / 359; Loss: 2.446694\n",
      "Epoch: 65; Batch 300 / 359; Loss: 2.587340\n",
      "Epoch: 66; Batch 100 / 359; Loss: 2.518447\n",
      "Epoch: 66; Batch 200 / 359; Loss: 2.621114\n",
      "Epoch: 66; Batch 300 / 359; Loss: 2.659750\n",
      "Epoch: 67; Batch 100 / 359; Loss: 2.549356\n",
      "Epoch: 67; Batch 200 / 359; Loss: 2.366504\n",
      "Epoch: 67; Batch 300 / 359; Loss: 2.531261\n",
      "Epoch: 68; Batch 100 / 359; Loss: 2.342626\n",
      "Epoch: 68; Batch 200 / 359; Loss: 2.665194\n",
      "Epoch: 68; Batch 300 / 359; Loss: 2.671200\n",
      "Epoch: 69; Batch 100 / 359; Loss: 2.374784\n",
      "Epoch: 69; Batch 200 / 359; Loss: 2.513028\n",
      "Epoch: 69; Batch 300 / 359; Loss: 2.448170\n",
      "Epoch: 70; Batch 100 / 359; Loss: 2.499286\n",
      "Epoch: 70; Batch 200 / 359; Loss: 2.583479\n",
      "Epoch: 70; Batch 300 / 359; Loss: 2.501554\n",
      "Epoch: 71; Batch 100 / 359; Loss: 2.290527\n",
      "Epoch: 71; Batch 200 / 359; Loss: 2.115355\n",
      "Epoch: 71; Batch 300 / 359; Loss: 2.287198\n",
      "Epoch: 72; Batch 100 / 359; Loss: 2.329731\n",
      "Epoch: 72; Batch 200 / 359; Loss: 2.514231\n",
      "Epoch: 72; Batch 300 / 359; Loss: 2.638100\n",
      "Epoch: 73; Batch 100 / 359; Loss: 2.369413\n",
      "Epoch: 73; Batch 200 / 359; Loss: 2.486564\n",
      "Epoch: 73; Batch 300 / 359; Loss: 2.546825\n",
      "Epoch: 74; Batch 100 / 359; Loss: 2.228855\n",
      "Epoch: 74; Batch 200 / 359; Loss: 2.346236\n",
      "Epoch: 74; Batch 300 / 359; Loss: 2.360716\n",
      "Epoch: 75; Batch 100 / 359; Loss: 2.323318\n",
      "Epoch: 75; Batch 200 / 359; Loss: 2.105852\n",
      "Epoch: 75; Batch 300 / 359; Loss: 2.391864\n",
      "Epoch: 76; Batch 100 / 359; Loss: 2.149441\n",
      "Epoch: 76; Batch 200 / 359; Loss: 2.280029\n",
      "Epoch: 76; Batch 300 / 359; Loss: 2.493869\n",
      "Epoch: 77; Batch 100 / 359; Loss: 2.143691\n",
      "Epoch: 77; Batch 200 / 359; Loss: 2.225285\n",
      "Epoch: 77; Batch 300 / 359; Loss: 2.416107\n",
      "Epoch: 78; Batch 100 / 359; Loss: 2.242033\n",
      "Epoch: 78; Batch 200 / 359; Loss: 2.290028\n",
      "Epoch: 78; Batch 300 / 359; Loss: 2.257030\n",
      "Epoch: 79; Batch 100 / 359; Loss: 2.315725\n",
      "Epoch: 79; Batch 200 / 359; Loss: 2.223692\n",
      "Epoch: 79; Batch 300 / 359; Loss: 2.240387\n",
      "Epoch: 80; Batch 100 / 359; Loss: 2.240410\n",
      "Epoch: 80; Batch 200 / 359; Loss: 2.402757\n",
      "Epoch: 80; Batch 300 / 359; Loss: 2.309047\n",
      "Epoch: 81; Batch 100 / 359; Loss: 2.138452\n",
      "Epoch: 81; Batch 200 / 359; Loss: 2.299458\n",
      "Epoch: 81; Batch 300 / 359; Loss: 2.467648\n",
      "Epoch: 82; Batch 100 / 359; Loss: 1.887404\n",
      "Epoch: 82; Batch 200 / 359; Loss: 1.970714\n",
      "Epoch: 82; Batch 300 / 359; Loss: 2.380419\n",
      "Epoch: 83; Batch 100 / 359; Loss: 2.141795\n",
      "Epoch: 83; Batch 200 / 359; Loss: 2.410801\n",
      "Epoch: 83; Batch 300 / 359; Loss: 2.325503\n",
      "Epoch: 84; Batch 100 / 359; Loss: 2.025794\n",
      "Epoch: 84; Batch 200 / 359; Loss: 1.987582\n",
      "Epoch: 84; Batch 300 / 359; Loss: 2.077564\n",
      "Epoch: 85; Batch 100 / 359; Loss: 2.139611\n",
      "Epoch: 85; Batch 200 / 359; Loss: 2.072563\n",
      "Epoch: 85; Batch 300 / 359; Loss: 2.355124\n",
      "Epoch: 86; Batch 100 / 359; Loss: 2.080709\n",
      "Epoch: 86; Batch 200 / 359; Loss: 1.946416\n",
      "Epoch: 86; Batch 300 / 359; Loss: 2.315781\n",
      "Epoch: 87; Batch 100 / 359; Loss: 2.080171\n",
      "Epoch: 87; Batch 200 / 359; Loss: 2.190786\n",
      "Epoch: 87; Batch 300 / 359; Loss: 2.103789\n",
      "Epoch: 88; Batch 100 / 359; Loss: 2.107399\n",
      "Epoch: 88; Batch 200 / 359; Loss: 2.265229\n",
      "Epoch: 88; Batch 300 / 359; Loss: 2.243458\n",
      "Epoch: 89; Batch 100 / 359; Loss: 2.175963\n",
      "Epoch: 89; Batch 200 / 359; Loss: 2.282012\n",
      "Epoch: 89; Batch 300 / 359; Loss: 2.238176\n",
      "Epoch: 90; Batch 100 / 359; Loss: 1.983859\n",
      "Epoch: 90; Batch 200 / 359; Loss: 2.053528\n",
      "Epoch: 90; Batch 300 / 359; Loss: 2.115387\n",
      "Epoch: 91; Batch 100 / 359; Loss: 2.024853\n",
      "Epoch: 91; Batch 200 / 359; Loss: 2.216630\n",
      "Epoch: 91; Batch 300 / 359; Loss: 2.195136\n",
      "Epoch: 92; Batch 100 / 359; Loss: 1.973947\n",
      "Epoch: 92; Batch 200 / 359; Loss: 2.011277\n",
      "Epoch: 92; Batch 300 / 359; Loss: 2.149081\n",
      "Epoch: 93; Batch 100 / 359; Loss: 2.257683\n",
      "Epoch: 93; Batch 200 / 359; Loss: 1.905393\n",
      "Epoch: 93; Batch 300 / 359; Loss: 2.096370\n",
      "Epoch: 94; Batch 100 / 359; Loss: 2.146356\n",
      "Epoch: 94; Batch 200 / 359; Loss: 2.083680\n",
      "Epoch: 94; Batch 300 / 359; Loss: 2.134278\n",
      "Epoch: 95; Batch 100 / 359; Loss: 2.111247\n",
      "Epoch: 95; Batch 200 / 359; Loss: 1.875764\n",
      "Epoch: 95; Batch 300 / 359; Loss: 2.210564\n",
      "Epoch: 96; Batch 100 / 359; Loss: 2.012786\n",
      "Epoch: 96; Batch 200 / 359; Loss: 2.065546\n",
      "Epoch: 96; Batch 300 / 359; Loss: 2.112054\n",
      "Epoch: 97; Batch 100 / 359; Loss: 2.040826\n",
      "Epoch: 97; Batch 200 / 359; Loss: 2.113115\n",
      "Epoch: 97; Batch 300 / 359; Loss: 1.722773\n",
      "Epoch: 98; Batch 100 / 359; Loss: 1.837979\n",
      "Epoch: 98; Batch 200 / 359; Loss: 1.978772\n",
      "Epoch: 98; Batch 300 / 359; Loss: 1.860267\n",
      "Epoch: 99; Batch 100 / 359; Loss: 1.822643\n",
      "Epoch: 99; Batch 200 / 359; Loss: 1.837317\n",
      "Epoch: 99; Batch 300 / 359; Loss: 1.983213\n",
      "Epoch: 100; Batch 100 / 359; Loss: 2.001331\n",
      "Epoch: 100; Batch 200 / 359; Loss: 1.730151\n",
      "Epoch: 100; Batch 300 / 359; Loss: 2.095959\n",
      "Epoch: 101; Batch 100 / 359; Loss: 2.051354\n",
      "Epoch: 101; Batch 200 / 359; Loss: 1.842565\n",
      "Epoch: 101; Batch 300 / 359; Loss: 1.986802\n",
      "Epoch: 102; Batch 100 / 359; Loss: 1.900849\n",
      "Epoch: 102; Batch 200 / 359; Loss: 2.056220\n",
      "Epoch: 102; Batch 300 / 359; Loss: 2.037274\n",
      "Epoch: 103; Batch 100 / 359; Loss: 1.893897\n",
      "Epoch: 103; Batch 200 / 359; Loss: 1.855743\n",
      "Epoch: 103; Batch 300 / 359; Loss: 1.776074\n",
      "Epoch: 104; Batch 100 / 359; Loss: 1.738805\n",
      "Epoch: 104; Batch 200 / 359; Loss: 2.059771\n",
      "Epoch: 104; Batch 300 / 359; Loss: 1.989678\n",
      "Epoch: 105; Batch 100 / 359; Loss: 1.812081\n",
      "Epoch: 105; Batch 200 / 359; Loss: 2.057968\n",
      "Epoch: 105; Batch 300 / 359; Loss: 2.035961\n",
      "Epoch: 106; Batch 100 / 359; Loss: 1.684727\n",
      "Epoch: 106; Batch 200 / 359; Loss: 2.035037\n",
      "Epoch: 106; Batch 300 / 359; Loss: 1.824345\n",
      "Epoch: 107; Batch 100 / 359; Loss: 1.895885\n",
      "Epoch: 107; Batch 200 / 359; Loss: 1.931600\n",
      "Epoch: 107; Batch 300 / 359; Loss: 1.711739\n",
      "Epoch: 108; Batch 100 / 359; Loss: 1.890477\n",
      "Epoch: 108; Batch 200 / 359; Loss: 1.899329\n",
      "Epoch: 108; Batch 300 / 359; Loss: 1.670624\n",
      "Epoch: 109; Batch 100 / 359; Loss: 1.886444\n",
      "Epoch: 109; Batch 200 / 359; Loss: 1.629689\n",
      "Epoch: 109; Batch 300 / 359; Loss: 1.593199\n",
      "Epoch: 110; Batch 100 / 359; Loss: 1.897525\n",
      "Epoch: 110; Batch 200 / 359; Loss: 1.878796\n",
      "Epoch: 110; Batch 300 / 359; Loss: 1.811022\n",
      "Epoch: 111; Batch 100 / 359; Loss: 1.774569\n",
      "Epoch: 111; Batch 200 / 359; Loss: 1.611681\n",
      "Epoch: 111; Batch 300 / 359; Loss: 1.699222\n",
      "Epoch: 112; Batch 100 / 359; Loss: 1.883512\n",
      "Epoch: 112; Batch 200 / 359; Loss: 1.831818\n",
      "Epoch: 112; Batch 300 / 359; Loss: 1.733629\n",
      "Epoch: 113; Batch 100 / 359; Loss: 1.834148\n",
      "Epoch: 113; Batch 200 / 359; Loss: 1.711347\n",
      "Epoch: 113; Batch 300 / 359; Loss: 1.723512\n",
      "Epoch: 114; Batch 100 / 359; Loss: 1.894194\n",
      "Epoch: 114; Batch 200 / 359; Loss: 1.928203\n",
      "Epoch: 114; Batch 300 / 359; Loss: 1.800136\n",
      "Epoch: 115; Batch 100 / 359; Loss: 1.786842\n",
      "Epoch: 115; Batch 200 / 359; Loss: 1.539769\n",
      "Epoch: 115; Batch 300 / 359; Loss: 1.971628\n",
      "Epoch: 116; Batch 100 / 359; Loss: 1.774804\n",
      "Epoch: 116; Batch 200 / 359; Loss: 1.763824\n",
      "Epoch: 116; Batch 300 / 359; Loss: 1.941649\n",
      "Epoch: 117; Batch 100 / 359; Loss: 1.818061\n",
      "Epoch: 117; Batch 200 / 359; Loss: 1.797425\n",
      "Epoch: 117; Batch 300 / 359; Loss: 1.584757\n",
      "Epoch: 118; Batch 100 / 359; Loss: 1.714592\n",
      "Epoch: 118; Batch 200 / 359; Loss: 1.950088\n",
      "Epoch: 118; Batch 300 / 359; Loss: 1.694320\n",
      "Epoch: 119; Batch 100 / 359; Loss: 1.686574\n",
      "Epoch: 119; Batch 200 / 359; Loss: 1.740764\n",
      "Epoch: 119; Batch 300 / 359; Loss: 1.764185\n",
      "Epoch: 120; Batch 100 / 359; Loss: 1.659617\n",
      "Epoch: 120; Batch 200 / 359; Loss: 1.677189\n",
      "Epoch: 120; Batch 300 / 359; Loss: 1.710664\n",
      "Epoch: 121; Batch 100 / 359; Loss: 1.632117\n",
      "Epoch: 121; Batch 200 / 359; Loss: 1.704960\n",
      "Epoch: 121; Batch 300 / 359; Loss: 1.665278\n",
      "Epoch: 122; Batch 100 / 359; Loss: 1.782660\n",
      "Epoch: 122; Batch 200 / 359; Loss: 1.432824\n",
      "Epoch: 122; Batch 300 / 359; Loss: 1.561494\n",
      "Epoch: 123; Batch 100 / 359; Loss: 1.727921\n",
      "Epoch: 123; Batch 200 / 359; Loss: 1.966175\n",
      "Epoch: 123; Batch 300 / 359; Loss: 1.841300\n",
      "Epoch: 124; Batch 100 / 359; Loss: 1.478947\n",
      "Epoch: 124; Batch 200 / 359; Loss: 1.709505\n",
      "Epoch: 124; Batch 300 / 359; Loss: 1.788493\n",
      "Epoch: 125; Batch 100 / 359; Loss: 1.688253\n",
      "Epoch: 125; Batch 200 / 359; Loss: 1.385996\n",
      "Epoch: 125; Batch 300 / 359; Loss: 1.937510\n",
      "Epoch: 126; Batch 100 / 359; Loss: 1.438978\n",
      "Epoch: 126; Batch 200 / 359; Loss: 1.566039\n",
      "Epoch: 126; Batch 300 / 359; Loss: 1.509420\n",
      "Epoch: 127; Batch 100 / 359; Loss: 1.803388\n",
      "Epoch: 127; Batch 200 / 359; Loss: 1.641569\n",
      "Epoch: 127; Batch 300 / 359; Loss: 1.729801\n",
      "Epoch: 128; Batch 100 / 359; Loss: 1.632611\n",
      "Epoch: 128; Batch 200 / 359; Loss: 1.641901\n",
      "Epoch: 128; Batch 300 / 359; Loss: 1.517231\n",
      "Epoch: 129; Batch 100 / 359; Loss: 1.330049\n",
      "Epoch: 129; Batch 200 / 359; Loss: 1.654573\n",
      "Epoch: 129; Batch 300 / 359; Loss: 1.871578\n",
      "Epoch: 130; Batch 100 / 359; Loss: 1.667616\n",
      "Epoch: 130; Batch 200 / 359; Loss: 1.800546\n",
      "Epoch: 130; Batch 300 / 359; Loss: 1.568842\n",
      "Epoch: 131; Batch 100 / 359; Loss: 1.473268\n",
      "Epoch: 131; Batch 200 / 359; Loss: 1.636611\n",
      "Epoch: 131; Batch 300 / 359; Loss: 1.632172\n",
      "Epoch: 132; Batch 100 / 359; Loss: 1.431222\n",
      "Epoch: 132; Batch 200 / 359; Loss: 1.396891\n",
      "Epoch: 132; Batch 300 / 359; Loss: 1.540579\n",
      "Epoch: 133; Batch 100 / 359; Loss: 1.505560\n",
      "Epoch: 133; Batch 200 / 359; Loss: 1.596606\n",
      "Epoch: 133; Batch 300 / 359; Loss: 1.656320\n",
      "Epoch: 134; Batch 100 / 359; Loss: 1.694946\n",
      "Epoch: 134; Batch 200 / 359; Loss: 1.542514\n",
      "Epoch: 134; Batch 300 / 359; Loss: 1.535865\n",
      "Epoch: 135; Batch 100 / 359; Loss: 1.638150\n",
      "Epoch: 135; Batch 200 / 359; Loss: 1.566609\n",
      "Epoch: 135; Batch 300 / 359; Loss: 1.433021\n",
      "Epoch: 136; Batch 100 / 359; Loss: 1.359186\n",
      "Epoch: 136; Batch 200 / 359; Loss: 1.613204\n",
      "Epoch: 136; Batch 300 / 359; Loss: 1.600950\n",
      "Epoch: 137; Batch 100 / 359; Loss: 1.497875\n",
      "Epoch: 137; Batch 200 / 359; Loss: 1.420009\n",
      "Epoch: 137; Batch 300 / 359; Loss: 1.460255\n",
      "Epoch: 138; Batch 100 / 359; Loss: 1.463760\n",
      "Epoch: 138; Batch 200 / 359; Loss: 1.650141\n",
      "Epoch: 138; Batch 300 / 359; Loss: 1.385104\n",
      "Epoch: 139; Batch 100 / 359; Loss: 1.492856\n",
      "Epoch: 139; Batch 200 / 359; Loss: 1.811170\n",
      "Epoch: 139; Batch 300 / 359; Loss: 1.547222\n",
      "Epoch: 140; Batch 100 / 359; Loss: 1.467519\n",
      "Epoch: 140; Batch 200 / 359; Loss: 1.529437\n",
      "Epoch: 140; Batch 300 / 359; Loss: 1.417138\n",
      "Epoch: 141; Batch 100 / 359; Loss: 1.513513\n",
      "Epoch: 141; Batch 200 / 359; Loss: 1.406600\n",
      "Epoch: 141; Batch 300 / 359; Loss: 1.499297\n",
      "Epoch: 142; Batch 100 / 359; Loss: 1.483671\n",
      "Epoch: 142; Batch 200 / 359; Loss: 1.541805\n",
      "Epoch: 142; Batch 300 / 359; Loss: 1.370366\n",
      "Epoch: 143; Batch 100 / 359; Loss: 1.424352\n",
      "Epoch: 143; Batch 200 / 359; Loss: 1.334848\n",
      "Epoch: 143; Batch 300 / 359; Loss: 1.217594\n",
      "Epoch: 144; Batch 100 / 359; Loss: 1.453239\n",
      "Epoch: 144; Batch 200 / 359; Loss: 1.646238\n",
      "Epoch: 144; Batch 300 / 359; Loss: 1.435980\n",
      "Epoch: 145; Batch 100 / 359; Loss: 1.335462\n",
      "Epoch: 145; Batch 200 / 359; Loss: 1.475068\n",
      "Epoch: 145; Batch 300 / 359; Loss: 1.169148\n",
      "Epoch: 146; Batch 100 / 359; Loss: 1.567210\n",
      "Epoch: 146; Batch 200 / 359; Loss: 1.446183\n",
      "Epoch: 146; Batch 300 / 359; Loss: 1.436722\n",
      "Epoch: 147; Batch 100 / 359; Loss: 1.543719\n",
      "Epoch: 147; Batch 200 / 359; Loss: 1.341944\n",
      "Epoch: 147; Batch 300 / 359; Loss: 1.554171\n",
      "Epoch: 148; Batch 100 / 359; Loss: 1.468608\n",
      "Epoch: 148; Batch 200 / 359; Loss: 1.527465\n",
      "Epoch: 148; Batch 300 / 359; Loss: 1.383847\n",
      "Epoch: 149; Batch 100 / 359; Loss: 1.476229\n",
      "Epoch: 149; Batch 200 / 359; Loss: 1.411181\n",
      "Epoch: 149; Batch 300 / 359; Loss: 1.363659\n",
      "Epoch: 150; Batch 100 / 359; Loss: 1.387444\n",
      "Epoch: 150; Batch 200 / 359; Loss: 1.196690\n",
      "Epoch: 150; Batch 300 / 359; Loss: 1.543114\n",
      "Epoch: 151; Batch 100 / 359; Loss: 1.320746\n",
      "Epoch: 151; Batch 200 / 359; Loss: 1.415968\n",
      "Epoch: 151; Batch 300 / 359; Loss: 1.271085\n",
      "Epoch: 152; Batch 100 / 359; Loss: 1.326085\n",
      "Epoch: 152; Batch 200 / 359; Loss: 1.309990\n",
      "Epoch: 152; Batch 300 / 359; Loss: 1.270148\n",
      "Epoch: 153; Batch 100 / 359; Loss: 1.361624\n",
      "Epoch: 153; Batch 200 / 359; Loss: 1.408535\n",
      "Epoch: 153; Batch 300 / 359; Loss: 1.319585\n",
      "Epoch: 154; Batch 100 / 359; Loss: 1.240566\n",
      "Epoch: 154; Batch 200 / 359; Loss: 1.154160\n",
      "Epoch: 154; Batch 300 / 359; Loss: 1.375216\n",
      "Epoch: 155; Batch 100 / 359; Loss: 1.169164\n",
      "Epoch: 155; Batch 200 / 359; Loss: 1.289330\n",
      "Epoch: 155; Batch 300 / 359; Loss: 1.392733\n",
      "Epoch: 156; Batch 100 / 359; Loss: 1.553418\n",
      "Epoch: 156; Batch 200 / 359; Loss: 1.346194\n",
      "Epoch: 156; Batch 300 / 359; Loss: 1.370462\n",
      "Epoch: 157; Batch 100 / 359; Loss: 1.236294\n",
      "Epoch: 157; Batch 200 / 359; Loss: 1.352031\n",
      "Epoch: 157; Batch 300 / 359; Loss: 1.327664\n",
      "Epoch: 158; Batch 100 / 359; Loss: 1.405034\n",
      "Epoch: 158; Batch 200 / 359; Loss: 1.536229\n",
      "Epoch: 158; Batch 300 / 359; Loss: 1.424420\n",
      "Epoch: 159; Batch 100 / 359; Loss: 1.289101\n",
      "Epoch: 159; Batch 200 / 359; Loss: 1.315729\n",
      "Epoch: 159; Batch 300 / 359; Loss: 1.246322\n",
      "Epoch: 160; Batch 100 / 359; Loss: 1.176218\n",
      "Epoch: 160; Batch 200 / 359; Loss: 1.178495\n",
      "Epoch: 160; Batch 300 / 359; Loss: 1.214448\n",
      "Epoch: 161; Batch 100 / 359; Loss: 1.187144\n",
      "Epoch: 161; Batch 200 / 359; Loss: 1.393890\n",
      "Epoch: 161; Batch 300 / 359; Loss: 1.298558\n",
      "Epoch: 162; Batch 100 / 359; Loss: 1.175238\n",
      "Epoch: 162; Batch 200 / 359; Loss: 1.411828\n",
      "Epoch: 162; Batch 300 / 359; Loss: 1.276439\n",
      "Epoch: 163; Batch 100 / 359; Loss: 1.394783\n",
      "Epoch: 163; Batch 200 / 359; Loss: 1.186375\n",
      "Epoch: 163; Batch 300 / 359; Loss: 1.341616\n",
      "Epoch: 164; Batch 100 / 359; Loss: 1.255741\n",
      "Epoch: 164; Batch 200 / 359; Loss: 1.616449\n",
      "Epoch: 164; Batch 300 / 359; Loss: 1.183675\n",
      "Epoch: 165; Batch 100 / 359; Loss: 1.130250\n",
      "Epoch: 165; Batch 200 / 359; Loss: 1.235917\n",
      "Epoch: 165; Batch 300 / 359; Loss: 1.165511\n",
      "Epoch: 166; Batch 100 / 359; Loss: 1.294006\n",
      "Epoch: 166; Batch 200 / 359; Loss: 1.429768\n",
      "Epoch: 166; Batch 300 / 359; Loss: 1.167379\n",
      "Epoch: 167; Batch 100 / 359; Loss: 1.181737\n",
      "Epoch: 167; Batch 200 / 359; Loss: 1.112219\n",
      "Epoch: 167; Batch 300 / 359; Loss: 1.134951\n",
      "Epoch: 168; Batch 100 / 359; Loss: 1.280337\n",
      "Epoch: 168; Batch 200 / 359; Loss: 1.305965\n",
      "Epoch: 168; Batch 300 / 359; Loss: 1.263612\n",
      "Epoch: 169; Batch 100 / 359; Loss: 1.146369\n",
      "Epoch: 169; Batch 200 / 359; Loss: 1.378413\n",
      "Epoch: 169; Batch 300 / 359; Loss: 1.302097\n",
      "Epoch: 170; Batch 100 / 359; Loss: 1.357393\n",
      "Epoch: 170; Batch 200 / 359; Loss: 1.303278\n",
      "Epoch: 170; Batch 300 / 359; Loss: 1.434048\n",
      "Epoch: 171; Batch 100 / 359; Loss: 1.262040\n",
      "Epoch: 171; Batch 200 / 359; Loss: 1.166862\n",
      "Epoch: 171; Batch 300 / 359; Loss: 1.182443\n",
      "Epoch: 172; Batch 100 / 359; Loss: 1.175828\n",
      "Epoch: 172; Batch 200 / 359; Loss: 1.212488\n",
      "Epoch: 172; Batch 300 / 359; Loss: 1.441839\n",
      "Epoch: 173; Batch 100 / 359; Loss: 1.201441\n",
      "Epoch: 173; Batch 200 / 359; Loss: 1.207533\n",
      "Epoch: 173; Batch 300 / 359; Loss: 1.108963\n",
      "Epoch: 174; Batch 100 / 359; Loss: 1.259486\n",
      "Epoch: 174; Batch 200 / 359; Loss: 1.107330\n",
      "Epoch: 174; Batch 300 / 359; Loss: 1.017970\n",
      "Epoch: 175; Batch 100 / 359; Loss: 1.159472\n",
      "Epoch: 175; Batch 200 / 359; Loss: 1.195064\n",
      "Epoch: 175; Batch 300 / 359; Loss: 0.988161\n",
      "Epoch: 176; Batch 100 / 359; Loss: 1.115044\n",
      "Epoch: 176; Batch 200 / 359; Loss: 1.178043\n",
      "Epoch: 176; Batch 300 / 359; Loss: 1.087401\n",
      "Epoch: 177; Batch 100 / 359; Loss: 1.082653\n",
      "Epoch: 177; Batch 200 / 359; Loss: 1.095859\n",
      "Epoch: 177; Batch 300 / 359; Loss: 1.196684\n",
      "Epoch: 178; Batch 100 / 359; Loss: 1.073310\n",
      "Epoch: 178; Batch 200 / 359; Loss: 1.143699\n",
      "Epoch: 178; Batch 300 / 359; Loss: 1.185629\n",
      "Epoch: 179; Batch 100 / 359; Loss: 1.158689\n",
      "Epoch: 179; Batch 200 / 359; Loss: 1.070058\n",
      "Epoch: 179; Batch 300 / 359; Loss: 1.286077\n",
      "Epoch: 180; Batch 100 / 359; Loss: 1.114028\n",
      "Epoch: 180; Batch 200 / 359; Loss: 1.064466\n",
      "Epoch: 180; Batch 300 / 359; Loss: 1.110424\n",
      "Epoch: 181; Batch 100 / 359; Loss: 0.928235\n",
      "Epoch: 181; Batch 200 / 359; Loss: 1.153283\n",
      "Epoch: 181; Batch 300 / 359; Loss: 1.182956\n",
      "Epoch: 182; Batch 100 / 359; Loss: 0.988740\n",
      "Epoch: 182; Batch 200 / 359; Loss: 1.113773\n",
      "Epoch: 182; Batch 300 / 359; Loss: 0.879501\n",
      "Epoch: 183; Batch 100 / 359; Loss: 0.969397\n",
      "Epoch: 183; Batch 200 / 359; Loss: 1.287772\n",
      "Epoch: 183; Batch 300 / 359; Loss: 1.180530\n",
      "Epoch: 184; Batch 100 / 359; Loss: 1.187307\n",
      "Epoch: 184; Batch 200 / 359; Loss: 1.117749\n",
      "Epoch: 184; Batch 300 / 359; Loss: 1.164551\n",
      "Epoch: 185; Batch 100 / 359; Loss: 1.231213\n",
      "Epoch: 185; Batch 200 / 359; Loss: 0.956299\n",
      "Epoch: 185; Batch 300 / 359; Loss: 1.025403\n",
      "Epoch: 186; Batch 100 / 359; Loss: 1.151068\n",
      "Epoch: 186; Batch 200 / 359; Loss: 1.121777\n",
      "Epoch: 186; Batch 300 / 359; Loss: 1.093449\n",
      "Epoch: 187; Batch 100 / 359; Loss: 1.174512\n",
      "Epoch: 187; Batch 200 / 359; Loss: 0.930448\n",
      "Epoch: 187; Batch 300 / 359; Loss: 1.009214\n",
      "Epoch: 188; Batch 100 / 359; Loss: 1.004746\n",
      "Epoch: 188; Batch 200 / 359; Loss: 1.087282\n",
      "Epoch: 188; Batch 300 / 359; Loss: 0.980553\n",
      "Epoch: 189; Batch 100 / 359; Loss: 1.074972\n",
      "Epoch: 189; Batch 200 / 359; Loss: 1.067606\n",
      "Epoch: 189; Batch 300 / 359; Loss: 1.000356\n",
      "Epoch: 190; Batch 100 / 359; Loss: 1.008590\n",
      "Epoch: 190; Batch 200 / 359; Loss: 1.056415\n",
      "Epoch: 190; Batch 300 / 359; Loss: 1.188874\n",
      "Epoch: 191; Batch 100 / 359; Loss: 0.992977\n",
      "Epoch: 191; Batch 200 / 359; Loss: 1.192979\n",
      "Epoch: 191; Batch 300 / 359; Loss: 1.269208\n",
      "Epoch: 192; Batch 100 / 359; Loss: 0.913711\n",
      "Epoch: 192; Batch 200 / 359; Loss: 0.980861\n",
      "Epoch: 192; Batch 300 / 359; Loss: 0.864291\n",
      "Epoch: 193; Batch 100 / 359; Loss: 0.878921\n",
      "Epoch: 193; Batch 200 / 359; Loss: 0.895548\n",
      "Epoch: 193; Batch 300 / 359; Loss: 1.217864\n",
      "Epoch: 194; Batch 100 / 359; Loss: 1.032518\n",
      "Epoch: 194; Batch 200 / 359; Loss: 1.070852\n",
      "Epoch: 194; Batch 300 / 359; Loss: 1.115081\n",
      "Epoch: 195; Batch 100 / 359; Loss: 0.898446\n",
      "Epoch: 195; Batch 200 / 359; Loss: 0.797112\n",
      "Epoch: 195; Batch 300 / 359; Loss: 0.884985\n",
      "Epoch: 196; Batch 100 / 359; Loss: 0.942482\n",
      "Epoch: 196; Batch 200 / 359; Loss: 0.896480\n",
      "Epoch: 196; Batch 300 / 359; Loss: 1.053945\n",
      "Epoch: 197; Batch 100 / 359; Loss: 1.010558\n",
      "Epoch: 197; Batch 200 / 359; Loss: 1.001548\n",
      "Epoch: 197; Batch 300 / 359; Loss: 1.062980\n",
      "Epoch: 198; Batch 100 / 359; Loss: 1.037125\n",
      "Epoch: 198; Batch 200 / 359; Loss: 0.980116\n",
      "Epoch: 198; Batch 300 / 359; Loss: 0.931067\n",
      "Epoch: 199; Batch 100 / 359; Loss: 1.029986\n",
      "Epoch: 199; Batch 200 / 359; Loss: 0.916195\n",
      "Epoch: 199; Batch 300 / 359; Loss: 1.140785\n",
      "Epoch: 200; Batch 100 / 359; Loss: 0.970454\n",
      "Epoch: 200; Batch 200 / 359; Loss: 0.942694\n",
      "Epoch: 200; Batch 300 / 359; Loss: 0.958122\n",
      "Epoch: 201; Batch 100 / 359; Loss: 1.043087\n",
      "Epoch: 201; Batch 200 / 359; Loss: 1.041666\n",
      "Epoch: 201; Batch 300 / 359; Loss: 1.150107\n",
      "Epoch: 202; Batch 100 / 359; Loss: 0.989777\n",
      "Epoch: 202; Batch 200 / 359; Loss: 0.991055\n",
      "Epoch: 202; Batch 300 / 359; Loss: 1.045261\n",
      "Epoch: 203; Batch 100 / 359; Loss: 0.909821\n",
      "Epoch: 203; Batch 200 / 359; Loss: 0.953543\n",
      "Epoch: 203; Batch 300 / 359; Loss: 1.055423\n",
      "Epoch: 204; Batch 100 / 359; Loss: 0.932289\n",
      "Epoch: 204; Batch 200 / 359; Loss: 0.934777\n",
      "Epoch: 204; Batch 300 / 359; Loss: 0.855071\n",
      "Epoch: 205; Batch 100 / 359; Loss: 0.770076\n",
      "Epoch: 205; Batch 200 / 359; Loss: 0.942382\n",
      "Epoch: 205; Batch 300 / 359; Loss: 1.201201\n",
      "Epoch: 206; Batch 100 / 359; Loss: 1.117628\n",
      "Epoch: 206; Batch 200 / 359; Loss: 0.956626\n",
      "Epoch: 206; Batch 300 / 359; Loss: 0.990412\n",
      "Epoch: 207; Batch 100 / 359; Loss: 0.899434\n",
      "Epoch: 207; Batch 200 / 359; Loss: 0.817760\n",
      "Epoch: 207; Batch 300 / 359; Loss: 0.910699\n",
      "Epoch: 208; Batch 100 / 359; Loss: 0.936980\n",
      "Epoch: 208; Batch 200 / 359; Loss: 0.901190\n",
      "Epoch: 208; Batch 300 / 359; Loss: 0.904614\n",
      "Epoch: 209; Batch 100 / 359; Loss: 0.928951\n",
      "Epoch: 209; Batch 200 / 359; Loss: 1.014451\n",
      "Epoch: 209; Batch 300 / 359; Loss: 0.861499\n",
      "Epoch: 210; Batch 100 / 359; Loss: 0.862546\n",
      "Epoch: 210; Batch 200 / 359; Loss: 0.959832\n",
      "Epoch: 210; Batch 300 / 359; Loss: 0.852670\n",
      "Epoch: 211; Batch 100 / 359; Loss: 0.944977\n",
      "Epoch: 211; Batch 200 / 359; Loss: 1.094114\n",
      "Epoch: 211; Batch 300 / 359; Loss: 0.812052\n",
      "Epoch: 212; Batch 100 / 359; Loss: 0.826981\n",
      "Epoch: 212; Batch 200 / 359; Loss: 0.855684\n",
      "Epoch: 212; Batch 300 / 359; Loss: 0.760077\n",
      "Epoch: 213; Batch 100 / 359; Loss: 0.777876\n",
      "Epoch: 213; Batch 200 / 359; Loss: 0.783561\n",
      "Epoch: 213; Batch 300 / 359; Loss: 0.871284\n",
      "Epoch: 214; Batch 100 / 359; Loss: 0.914107\n",
      "Epoch: 214; Batch 200 / 359; Loss: 0.756804\n",
      "Epoch: 214; Batch 300 / 359; Loss: 0.918429\n",
      "Epoch: 215; Batch 100 / 359; Loss: 0.837168\n",
      "Epoch: 215; Batch 200 / 359; Loss: 0.741077\n",
      "Epoch: 215; Batch 300 / 359; Loss: 0.866668\n",
      "Epoch: 216; Batch 100 / 359; Loss: 0.804187\n",
      "Epoch: 216; Batch 200 / 359; Loss: 0.967949\n",
      "Epoch: 216; Batch 300 / 359; Loss: 0.788195\n",
      "Epoch: 217; Batch 100 / 359; Loss: 0.850321\n",
      "Epoch: 217; Batch 200 / 359; Loss: 0.839528\n",
      "Epoch: 217; Batch 300 / 359; Loss: 0.784404\n",
      "Epoch: 218; Batch 100 / 359; Loss: 0.909510\n",
      "Epoch: 218; Batch 200 / 359; Loss: 0.745802\n",
      "Epoch: 218; Batch 300 / 359; Loss: 0.907794\n",
      "Epoch: 219; Batch 100 / 359; Loss: 0.908920\n",
      "Epoch: 219; Batch 200 / 359; Loss: 0.894107\n",
      "Epoch: 219; Batch 300 / 359; Loss: 0.853577\n",
      "Epoch: 220; Batch 100 / 359; Loss: 0.928313\n",
      "Epoch: 220; Batch 200 / 359; Loss: 0.921587\n",
      "Epoch: 220; Batch 300 / 359; Loss: 0.798761\n",
      "Epoch: 221; Batch 100 / 359; Loss: 0.846954\n",
      "Epoch: 221; Batch 200 / 359; Loss: 0.986384\n",
      "Epoch: 221; Batch 300 / 359; Loss: 0.767891\n",
      "Epoch: 222; Batch 100 / 359; Loss: 0.782223\n",
      "Epoch: 222; Batch 200 / 359; Loss: 0.703911\n",
      "Epoch: 222; Batch 300 / 359; Loss: 0.706603\n",
      "Epoch: 223; Batch 100 / 359; Loss: 0.844685\n",
      "Epoch: 223; Batch 200 / 359; Loss: 0.928983\n",
      "Epoch: 223; Batch 300 / 359; Loss: 0.626517\n",
      "Epoch: 224; Batch 100 / 359; Loss: 0.873434\n",
      "Epoch: 224; Batch 200 / 359; Loss: 0.705871\n",
      "Epoch: 224; Batch 300 / 359; Loss: 0.656816\n",
      "Epoch: 225; Batch 100 / 359; Loss: 0.985988\n",
      "Epoch: 225; Batch 200 / 359; Loss: 0.969449\n",
      "Epoch: 225; Batch 300 / 359; Loss: 0.665554\n",
      "Epoch: 226; Batch 100 / 359; Loss: 0.846553\n",
      "Epoch: 226; Batch 200 / 359; Loss: 0.736814\n",
      "Epoch: 226; Batch 300 / 359; Loss: 0.816420\n",
      "Epoch: 227; Batch 100 / 359; Loss: 0.861510\n",
      "Epoch: 227; Batch 200 / 359; Loss: 0.794769\n",
      "Epoch: 227; Batch 300 / 359; Loss: 0.797411\n",
      "Epoch: 228; Batch 100 / 359; Loss: 0.792494\n",
      "Epoch: 228; Batch 200 / 359; Loss: 0.830461\n",
      "Epoch: 228; Batch 300 / 359; Loss: 0.813558\n",
      "Epoch: 229; Batch 100 / 359; Loss: 0.733429\n",
      "Epoch: 229; Batch 200 / 359; Loss: 0.793038\n",
      "Epoch: 229; Batch 300 / 359; Loss: 0.691471\n",
      "Epoch: 230; Batch 100 / 359; Loss: 0.895245\n",
      "Epoch: 230; Batch 200 / 359; Loss: 0.875838\n",
      "Epoch: 230; Batch 300 / 359; Loss: 0.905607\n",
      "Epoch: 231; Batch 100 / 359; Loss: 0.785015\n",
      "Epoch: 231; Batch 200 / 359; Loss: 0.831315\n",
      "Epoch: 231; Batch 300 / 359; Loss: 0.850963\n",
      "Epoch: 232; Batch 100 / 359; Loss: 0.684510\n",
      "Epoch: 232; Batch 200 / 359; Loss: 0.568882\n",
      "Epoch: 232; Batch 300 / 359; Loss: 0.824234\n",
      "Epoch: 233; Batch 100 / 359; Loss: 0.632351\n",
      "Epoch: 233; Batch 200 / 359; Loss: 0.677424\n",
      "Epoch: 233; Batch 300 / 359; Loss: 0.606803\n",
      "Epoch: 234; Batch 100 / 359; Loss: 0.822485\n",
      "Epoch: 234; Batch 200 / 359; Loss: 0.925306\n",
      "Epoch: 234; Batch 300 / 359; Loss: 0.822748\n",
      "Epoch: 235; Batch 100 / 359; Loss: 0.768934\n",
      "Epoch: 235; Batch 200 / 359; Loss: 0.859686\n",
      "Epoch: 235; Batch 300 / 359; Loss: 0.687034\n",
      "Epoch: 236; Batch 100 / 359; Loss: 0.669501\n",
      "Epoch: 236; Batch 200 / 359; Loss: 0.590543\n",
      "Epoch: 236; Batch 300 / 359; Loss: 0.661942\n",
      "Epoch: 237; Batch 100 / 359; Loss: 0.861038\n",
      "Epoch: 237; Batch 200 / 359; Loss: 0.671275\n",
      "Epoch: 237; Batch 300 / 359; Loss: 0.918633\n",
      "Epoch: 238; Batch 100 / 359; Loss: 0.687877\n",
      "Epoch: 238; Batch 200 / 359; Loss: 0.673204\n",
      "Epoch: 238; Batch 300 / 359; Loss: 0.686509\n",
      "Epoch: 239; Batch 100 / 359; Loss: 0.785963\n",
      "Epoch: 239; Batch 200 / 359; Loss: 0.483000\n",
      "Epoch: 239; Batch 300 / 359; Loss: 0.764438\n",
      "Epoch: 240; Batch 100 / 359; Loss: 0.681762\n",
      "Epoch: 240; Batch 200 / 359; Loss: 0.577853\n",
      "Epoch: 240; Batch 300 / 359; Loss: 0.677503\n",
      "Epoch: 241; Batch 100 / 359; Loss: 0.713158\n",
      "Epoch: 241; Batch 200 / 359; Loss: 0.932383\n",
      "Epoch: 241; Batch 300 / 359; Loss: 0.672098\n",
      "Epoch: 242; Batch 100 / 359; Loss: 0.719353\n",
      "Epoch: 242; Batch 200 / 359; Loss: 0.683502\n",
      "Epoch: 242; Batch 300 / 359; Loss: 0.674152\n",
      "Epoch: 243; Batch 100 / 359; Loss: 0.654527\n",
      "Epoch: 243; Batch 200 / 359; Loss: 0.681688\n",
      "Epoch: 243; Batch 300 / 359; Loss: 0.650455\n",
      "Epoch: 244; Batch 100 / 359; Loss: 0.593300\n",
      "Epoch: 244; Batch 200 / 359; Loss: 0.777138\n",
      "Epoch: 244; Batch 300 / 359; Loss: 0.671744\n",
      "Epoch: 245; Batch 100 / 359; Loss: 0.668851\n",
      "Epoch: 245; Batch 200 / 359; Loss: 0.670568\n",
      "Epoch: 245; Batch 300 / 359; Loss: 0.665203\n",
      "Epoch: 246; Batch 100 / 359; Loss: 0.551866\n",
      "Epoch: 246; Batch 200 / 359; Loss: 1.087269\n",
      "Epoch: 246; Batch 300 / 359; Loss: 0.658246\n",
      "Epoch: 247; Batch 100 / 359; Loss: 0.543431\n",
      "Epoch: 247; Batch 200 / 359; Loss: 0.718192\n",
      "Epoch: 247; Batch 300 / 359; Loss: 0.748806\n",
      "Epoch: 248; Batch 100 / 359; Loss: 0.604514\n",
      "Epoch: 248; Batch 200 / 359; Loss: 0.573486\n",
      "Epoch: 248; Batch 300 / 359; Loss: 0.703145\n",
      "Epoch: 249; Batch 100 / 359; Loss: 0.617653\n",
      "Epoch: 249; Batch 200 / 359; Loss: 0.692231\n",
      "Epoch: 249; Batch 300 / 359; Loss: 0.631542\n",
      "Epoch: 250; Batch 100 / 359; Loss: 0.651710\n",
      "Epoch: 250; Batch 200 / 359; Loss: 0.752219\n",
      "Epoch: 250; Batch 300 / 359; Loss: 0.680802\n",
      "Epoch: 251; Batch 100 / 359; Loss: 0.715885\n",
      "Epoch: 251; Batch 200 / 359; Loss: 0.696703\n",
      "Epoch: 251; Batch 300 / 359; Loss: 0.613965\n",
      "Epoch: 252; Batch 100 / 359; Loss: 0.681195\n",
      "Epoch: 252; Batch 200 / 359; Loss: 0.622588\n",
      "Epoch: 252; Batch 300 / 359; Loss: 0.665115\n",
      "Epoch: 253; Batch 100 / 359; Loss: 0.778879\n",
      "Epoch: 253; Batch 200 / 359; Loss: 0.656054\n",
      "Epoch: 253; Batch 300 / 359; Loss: 0.704470\n",
      "Epoch: 254; Batch 100 / 359; Loss: 0.524882\n",
      "Epoch: 254; Batch 200 / 359; Loss: 0.698061\n",
      "Epoch: 254; Batch 300 / 359; Loss: 0.896612\n",
      "Epoch: 255; Batch 100 / 359; Loss: 0.748260\n",
      "Epoch: 255; Batch 200 / 359; Loss: 0.496289\n",
      "Epoch: 255; Batch 300 / 359; Loss: 0.565520\n",
      "Epoch: 256; Batch 100 / 359; Loss: 0.648783\n",
      "Epoch: 256; Batch 200 / 359; Loss: 0.715619\n",
      "Epoch: 256; Batch 300 / 359; Loss: 0.718907\n",
      "Epoch: 257; Batch 100 / 359; Loss: 0.665941\n",
      "Epoch: 257; Batch 200 / 359; Loss: 0.849912\n",
      "Epoch: 257; Batch 300 / 359; Loss: 0.557258\n",
      "Epoch: 258; Batch 100 / 359; Loss: 0.643185\n",
      "Epoch: 258; Batch 200 / 359; Loss: 0.598966\n",
      "Epoch: 258; Batch 300 / 359; Loss: 0.576927\n",
      "Epoch: 259; Batch 100 / 359; Loss: 0.661444\n",
      "Epoch: 259; Batch 200 / 359; Loss: 0.564541\n",
      "Epoch: 259; Batch 300 / 359; Loss: 0.669503\n",
      "Epoch: 260; Batch 100 / 359; Loss: 0.745892\n",
      "Epoch: 260; Batch 200 / 359; Loss: 0.680649\n",
      "Epoch: 260; Batch 300 / 359; Loss: 0.707075\n",
      "Epoch: 261; Batch 100 / 359; Loss: 0.763148\n",
      "Epoch: 261; Batch 200 / 359; Loss: 0.724117\n",
      "Epoch: 261; Batch 300 / 359; Loss: 0.674677\n",
      "Epoch: 262; Batch 100 / 359; Loss: 0.446748\n",
      "Epoch: 262; Batch 200 / 359; Loss: 0.493313\n",
      "Epoch: 262; Batch 300 / 359; Loss: 0.713068\n",
      "Epoch: 263; Batch 100 / 359; Loss: 0.540376\n",
      "Epoch: 263; Batch 200 / 359; Loss: 0.509575\n",
      "Epoch: 263; Batch 300 / 359; Loss: 0.603314\n",
      "Epoch: 264; Batch 100 / 359; Loss: 0.660562\n",
      "Epoch: 264; Batch 200 / 359; Loss: 0.497804\n",
      "Epoch: 264; Batch 300 / 359; Loss: 0.593918\n",
      "Epoch: 265; Batch 100 / 359; Loss: 0.681691\n",
      "Epoch: 265; Batch 200 / 359; Loss: 0.610109\n",
      "Epoch: 265; Batch 300 / 359; Loss: 0.726119\n",
      "Epoch: 266; Batch 100 / 359; Loss: 0.495256\n",
      "Epoch: 266; Batch 200 / 359; Loss: 0.529634\n",
      "Epoch: 266; Batch 300 / 359; Loss: 0.590321\n",
      "Epoch: 267; Batch 100 / 359; Loss: 0.623187\n",
      "Epoch: 267; Batch 200 / 359; Loss: 0.635735\n",
      "Epoch: 267; Batch 300 / 359; Loss: 0.695457\n",
      "Epoch: 268; Batch 100 / 359; Loss: 0.510035\n",
      "Epoch: 268; Batch 200 / 359; Loss: 0.486942\n",
      "Epoch: 268; Batch 300 / 359; Loss: 0.650241\n",
      "Epoch: 269; Batch 100 / 359; Loss: 0.630239\n",
      "Epoch: 269; Batch 200 / 359; Loss: 0.548053\n",
      "Epoch: 269; Batch 300 / 359; Loss: 0.642956\n",
      "Epoch: 270; Batch 100 / 359; Loss: 0.446232\n",
      "Epoch: 270; Batch 200 / 359; Loss: 0.629888\n",
      "Epoch: 270; Batch 300 / 359; Loss: 0.650850\n",
      "Epoch: 271; Batch 100 / 359; Loss: 0.631362\n",
      "Epoch: 271; Batch 200 / 359; Loss: 0.513427\n",
      "Epoch: 271; Batch 300 / 359; Loss: 0.579577\n",
      "Epoch: 272; Batch 100 / 359; Loss: 0.636190\n",
      "Epoch: 272; Batch 200 / 359; Loss: 0.484738\n",
      "Epoch: 272; Batch 300 / 359; Loss: 0.488813\n",
      "Epoch: 273; Batch 100 / 359; Loss: 0.576964\n",
      "Epoch: 273; Batch 200 / 359; Loss: 0.613729\n",
      "Epoch: 273; Batch 300 / 359; Loss: 0.497087\n",
      "Epoch: 274; Batch 100 / 359; Loss: 0.440169\n",
      "Epoch: 274; Batch 200 / 359; Loss: 0.532060\n",
      "Epoch: 274; Batch 300 / 359; Loss: 0.657846\n",
      "Epoch: 275; Batch 100 / 359; Loss: 0.638766\n",
      "Epoch: 275; Batch 200 / 359; Loss: 0.551834\n",
      "Epoch: 275; Batch 300 / 359; Loss: 0.594960\n",
      "Epoch: 276; Batch 100 / 359; Loss: 0.710790\n",
      "Epoch: 276; Batch 200 / 359; Loss: 0.625970\n",
      "Epoch: 276; Batch 300 / 359; Loss: 0.481525\n",
      "Epoch: 277; Batch 100 / 359; Loss: 0.554888\n",
      "Epoch: 277; Batch 200 / 359; Loss: 0.560713\n",
      "Epoch: 277; Batch 300 / 359; Loss: 0.626628\n",
      "Epoch: 278; Batch 100 / 359; Loss: 0.539215\n",
      "Epoch: 278; Batch 200 / 359; Loss: 0.578346\n",
      "Epoch: 278; Batch 300 / 359; Loss: 0.617867\n",
      "Epoch: 279; Batch 100 / 359; Loss: 0.488726\n",
      "Epoch: 279; Batch 200 / 359; Loss: 0.468453\n",
      "Epoch: 279; Batch 300 / 359; Loss: 0.427029\n",
      "Epoch: 280; Batch 100 / 359; Loss: 0.538127\n",
      "Epoch: 280; Batch 200 / 359; Loss: 0.555184\n",
      "Epoch: 280; Batch 300 / 359; Loss: 0.572718\n",
      "Epoch: 281; Batch 100 / 359; Loss: 0.495055\n",
      "Epoch: 281; Batch 200 / 359; Loss: 0.533648\n",
      "Epoch: 281; Batch 300 / 359; Loss: 0.591112\n",
      "Epoch: 282; Batch 100 / 359; Loss: 0.460138\n",
      "Epoch: 282; Batch 200 / 359; Loss: 0.560865\n",
      "Epoch: 282; Batch 300 / 359; Loss: 0.800057\n",
      "Epoch: 283; Batch 100 / 359; Loss: 0.495940\n",
      "Epoch: 283; Batch 200 / 359; Loss: 1.023075\n",
      "Epoch: 283; Batch 300 / 359; Loss: 0.756221\n",
      "Epoch: 284; Batch 100 / 359; Loss: 0.680775\n",
      "Epoch: 284; Batch 200 / 359; Loss: 0.496055\n",
      "Epoch: 284; Batch 300 / 359; Loss: 0.466772\n",
      "Epoch: 285; Batch 100 / 359; Loss: 0.447056\n",
      "Epoch: 285; Batch 200 / 359; Loss: 0.450438\n",
      "Epoch: 285; Batch 300 / 359; Loss: 0.497511\n",
      "Epoch: 286; Batch 100 / 359; Loss: 0.429513\n",
      "Epoch: 286; Batch 200 / 359; Loss: 0.412830\n",
      "Epoch: 286; Batch 300 / 359; Loss: 0.590603\n",
      "Epoch: 287; Batch 100 / 359; Loss: 0.653912\n",
      "Epoch: 287; Batch 200 / 359; Loss: 0.406829\n",
      "Epoch: 287; Batch 300 / 359; Loss: 0.413655\n",
      "Epoch: 288; Batch 100 / 359; Loss: 0.487834\n",
      "Epoch: 288; Batch 200 / 359; Loss: 0.432859\n",
      "Epoch: 288; Batch 300 / 359; Loss: 0.396421\n",
      "Epoch: 289; Batch 100 / 359; Loss: 0.489932\n",
      "Epoch: 289; Batch 200 / 359; Loss: 0.403264\n",
      "Epoch: 289; Batch 300 / 359; Loss: 0.495023\n",
      "Epoch: 290; Batch 100 / 359; Loss: 0.455712\n",
      "Epoch: 290; Batch 200 / 359; Loss: 0.459539\n",
      "Epoch: 290; Batch 300 / 359; Loss: 0.393061\n",
      "Epoch: 291; Batch 100 / 359; Loss: 0.489714\n",
      "Epoch: 291; Batch 200 / 359; Loss: 0.469342\n",
      "Epoch: 291; Batch 300 / 359; Loss: 0.493066\n",
      "Epoch: 292; Batch 100 / 359; Loss: 0.536412\n",
      "Epoch: 292; Batch 200 / 359; Loss: 0.481576\n",
      "Epoch: 292; Batch 300 / 359; Loss: 0.490202\n",
      "Epoch: 293; Batch 100 / 359; Loss: 0.422553\n",
      "Epoch: 293; Batch 200 / 359; Loss: 0.437722\n",
      "Epoch: 293; Batch 300 / 359; Loss: 0.669752\n",
      "Epoch: 294; Batch 100 / 359; Loss: 0.600096\n",
      "Epoch: 294; Batch 200 / 359; Loss: 0.465611\n",
      "Epoch: 294; Batch 300 / 359; Loss: 0.491106\n",
      "Epoch: 295; Batch 100 / 359; Loss: 0.501283\n",
      "Epoch: 295; Batch 200 / 359; Loss: 0.522075\n",
      "Epoch: 295; Batch 300 / 359; Loss: 0.558605\n",
      "Epoch: 296; Batch 100 / 359; Loss: 0.503538\n",
      "Epoch: 296; Batch 200 / 359; Loss: 0.432858\n",
      "Epoch: 296; Batch 300 / 359; Loss: 0.457318\n",
      "Epoch: 297; Batch 100 / 359; Loss: 0.448956\n",
      "Epoch: 297; Batch 200 / 359; Loss: 0.569237\n",
      "Epoch: 297; Batch 300 / 359; Loss: 0.468608\n",
      "Epoch: 298; Batch 100 / 359; Loss: 0.472083\n",
      "Epoch: 298; Batch 200 / 359; Loss: 0.456843\n",
      "Epoch: 298; Batch 300 / 359; Loss: 0.434552\n",
      "Epoch: 299; Batch 100 / 359; Loss: 0.480014\n",
      "Epoch: 299; Batch 200 / 359; Loss: 0.488769\n",
      "Epoch: 299; Batch 300 / 359; Loss: 0.512821\n",
      "Epoch: 300; Batch 100 / 359; Loss: 0.350946\n",
      "Epoch: 300; Batch 200 / 359; Loss: 0.462232\n",
      "Epoch: 300; Batch 300 / 359; Loss: 0.483801\n"
     ]
    }
   ],
   "source": [
    "model = MusicLSTM(input_len=network_input.shape[2], hidden_size=256, num_classes=n_vocab, num_layers=2)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0003)\n",
    "\n",
    "losses = train(num_epochs=400, model=model, loss_func=loss_function, optimizer=optimizer, train_dataloader=dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"models/music_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1; Batch 100 / 359; Loss: 0.477301\n",
      "Epoch: 1; Batch 200 / 359; Loss: 0.546556\n",
      "Epoch: 1; Batch 300 / 359; Loss: 0.518844\n",
      "Epoch: 2; Batch 100 / 359; Loss: 0.486598\n",
      "Epoch: 2; Batch 200 / 359; Loss: 0.489876\n",
      "Epoch: 2; Batch 300 / 359; Loss: 0.508604\n",
      "Epoch: 3; Batch 100 / 359; Loss: 0.478998\n",
      "Epoch: 3; Batch 200 / 359; Loss: 0.474869\n",
      "Epoch: 3; Batch 300 / 359; Loss: 0.470879\n",
      "Epoch: 4; Batch 100 / 359; Loss: 0.498757\n",
      "Epoch: 4; Batch 200 / 359; Loss: 0.435737\n",
      "Epoch: 4; Batch 300 / 359; Loss: 0.361770\n",
      "Epoch: 5; Batch 100 / 359; Loss: 0.489261\n",
      "Epoch: 5; Batch 200 / 359; Loss: 0.485608\n",
      "Epoch: 5; Batch 300 / 359; Loss: 0.373657\n",
      "Epoch: 6; Batch 100 / 359; Loss: 0.461851\n",
      "Epoch: 6; Batch 200 / 359; Loss: 0.454362\n",
      "Epoch: 6; Batch 300 / 359; Loss: 0.591164\n",
      "Epoch: 7; Batch 100 / 359; Loss: 0.416573\n",
      "Epoch: 7; Batch 200 / 359; Loss: 0.447249\n",
      "Epoch: 7; Batch 300 / 359; Loss: 0.397537\n",
      "Epoch: 8; Batch 100 / 359; Loss: 0.398073\n",
      "Epoch: 8; Batch 200 / 359; Loss: 0.604189\n",
      "Epoch: 8; Batch 300 / 359; Loss: 0.586318\n",
      "Epoch: 9; Batch 100 / 359; Loss: 0.373158\n",
      "Epoch: 9; Batch 200 / 359; Loss: 0.440949\n",
      "Epoch: 9; Batch 300 / 359; Loss: 0.478041\n",
      "Epoch: 10; Batch 100 / 359; Loss: 0.347509\n",
      "Epoch: 10; Batch 200 / 359; Loss: 0.439283\n",
      "Epoch: 10; Batch 300 / 359; Loss: 0.431898\n",
      "Epoch: 11; Batch 100 / 359; Loss: 0.348410\n",
      "Epoch: 11; Batch 200 / 359; Loss: 0.599308\n",
      "Epoch: 11; Batch 300 / 359; Loss: 0.392275\n",
      "Epoch: 12; Batch 100 / 359; Loss: 0.416969\n",
      "Epoch: 12; Batch 200 / 359; Loss: 0.374479\n",
      "Epoch: 12; Batch 300 / 359; Loss: 0.444553\n",
      "Epoch: 13; Batch 100 / 359; Loss: 0.424273\n",
      "Epoch: 13; Batch 200 / 359; Loss: 0.436489\n",
      "Epoch: 13; Batch 300 / 359; Loss: 0.534390\n",
      "Epoch: 14; Batch 100 / 359; Loss: 0.461401\n",
      "Epoch: 14; Batch 200 / 359; Loss: 0.401414\n",
      "Epoch: 14; Batch 300 / 359; Loss: 0.358876\n",
      "Epoch: 15; Batch 100 / 359; Loss: 0.389361\n",
      "Epoch: 15; Batch 200 / 359; Loss: 0.396684\n",
      "Epoch: 15; Batch 300 / 359; Loss: 0.471192\n",
      "Epoch: 16; Batch 100 / 359; Loss: 0.318991\n",
      "Epoch: 16; Batch 200 / 359; Loss: 0.449630\n",
      "Epoch: 16; Batch 300 / 359; Loss: 0.418563\n",
      "Epoch: 17; Batch 100 / 359; Loss: 0.373346\n",
      "Epoch: 17; Batch 200 / 359; Loss: 0.462204\n",
      "Epoch: 17; Batch 300 / 359; Loss: 0.547080\n",
      "Epoch: 18; Batch 100 / 359; Loss: 0.437771\n",
      "Epoch: 18; Batch 200 / 359; Loss: 0.399369\n",
      "Epoch: 18; Batch 300 / 359; Loss: 0.418650\n",
      "Epoch: 19; Batch 100 / 359; Loss: 0.314364\n",
      "Epoch: 19; Batch 200 / 359; Loss: 0.465128\n",
      "Epoch: 19; Batch 300 / 359; Loss: 0.596415\n",
      "Epoch: 20; Batch 100 / 359; Loss: 0.788016\n",
      "Epoch: 20; Batch 200 / 359; Loss: 0.540791\n",
      "Epoch: 20; Batch 300 / 359; Loss: 0.417776\n",
      "Epoch: 21; Batch 100 / 359; Loss: 0.300051\n",
      "Epoch: 21; Batch 200 / 359; Loss: 0.401393\n",
      "Epoch: 21; Batch 300 / 359; Loss: 0.394266\n",
      "Epoch: 22; Batch 100 / 359; Loss: 0.374915\n",
      "Epoch: 22; Batch 200 / 359; Loss: 0.429697\n",
      "Epoch: 22; Batch 300 / 359; Loss: 0.399590\n",
      "Epoch: 23; Batch 100 / 359; Loss: 0.338350\n",
      "Epoch: 23; Batch 200 / 359; Loss: 0.331855\n",
      "Epoch: 23; Batch 300 / 359; Loss: 0.449114\n",
      "Epoch: 24; Batch 100 / 359; Loss: 0.390029\n",
      "Epoch: 24; Batch 200 / 359; Loss: 0.426447\n",
      "Epoch: 24; Batch 300 / 359; Loss: 0.425233\n",
      "Epoch: 25; Batch 100 / 359; Loss: 0.432175\n",
      "Epoch: 25; Batch 200 / 359; Loss: 0.420046\n",
      "Epoch: 25; Batch 300 / 359; Loss: 0.438922\n",
      "Epoch: 26; Batch 100 / 359; Loss: 0.436879\n",
      "Epoch: 26; Batch 200 / 359; Loss: 0.349257\n",
      "Epoch: 26; Batch 300 / 359; Loss: 0.339544\n",
      "Epoch: 27; Batch 100 / 359; Loss: 0.377029\n",
      "Epoch: 27; Batch 200 / 359; Loss: 0.382020\n",
      "Epoch: 27; Batch 300 / 359; Loss: 0.359945\n",
      "Epoch: 28; Batch 100 / 359; Loss: 0.431778\n",
      "Epoch: 28; Batch 200 / 359; Loss: 0.393336\n",
      "Epoch: 28; Batch 300 / 359; Loss: 0.340144\n",
      "Epoch: 29; Batch 100 / 359; Loss: 0.410871\n",
      "Epoch: 29; Batch 200 / 359; Loss: 0.454657\n",
      "Epoch: 29; Batch 300 / 359; Loss: 0.359067\n",
      "Epoch: 30; Batch 100 / 359; Loss: 0.366577\n",
      "Epoch: 30; Batch 200 / 359; Loss: 0.321664\n",
      "Epoch: 30; Batch 300 / 359; Loss: 0.539823\n",
      "Epoch: 31; Batch 100 / 359; Loss: 0.393299\n",
      "Epoch: 31; Batch 200 / 359; Loss: 0.345245\n",
      "Epoch: 31; Batch 300 / 359; Loss: 0.327292\n",
      "Epoch: 32; Batch 100 / 359; Loss: 0.348013\n",
      "Epoch: 32; Batch 200 / 359; Loss: 0.389070\n",
      "Epoch: 32; Batch 300 / 359; Loss: 0.426873\n",
      "Epoch: 33; Batch 100 / 359; Loss: 0.342409\n",
      "Epoch: 33; Batch 200 / 359; Loss: 0.358023\n",
      "Epoch: 33; Batch 300 / 359; Loss: 0.371038\n",
      "Epoch: 34; Batch 100 / 359; Loss: 0.349258\n",
      "Epoch: 34; Batch 200 / 359; Loss: 0.349597\n",
      "Epoch: 34; Batch 300 / 359; Loss: 0.371889\n",
      "Epoch: 35; Batch 100 / 359; Loss: 0.384224\n",
      "Epoch: 35; Batch 200 / 359; Loss: 0.315121\n",
      "Epoch: 35; Batch 300 / 359; Loss: 0.363769\n",
      "Epoch: 36; Batch 100 / 359; Loss: 0.326238\n",
      "Epoch: 36; Batch 200 / 359; Loss: 0.418939\n",
      "Epoch: 36; Batch 300 / 359; Loss: 0.503499\n",
      "Epoch: 37; Batch 100 / 359; Loss: 0.335993\n",
      "Epoch: 37; Batch 200 / 359; Loss: 0.418789\n",
      "Epoch: 37; Batch 300 / 359; Loss: 0.312497\n",
      "Epoch: 38; Batch 100 / 359; Loss: 0.290100\n",
      "Epoch: 38; Batch 200 / 359; Loss: 0.318733\n",
      "Epoch: 38; Batch 300 / 359; Loss: 0.294430\n",
      "Epoch: 39; Batch 100 / 359; Loss: 0.304782\n",
      "Epoch: 39; Batch 200 / 359; Loss: 0.740042\n",
      "Epoch: 39; Batch 300 / 359; Loss: 0.483987\n",
      "Epoch: 40; Batch 100 / 359; Loss: 0.384834\n",
      "Epoch: 40; Batch 200 / 359; Loss: 0.387439\n",
      "Epoch: 40; Batch 300 / 359; Loss: 0.421176\n",
      "Epoch: 41; Batch 100 / 359; Loss: 0.299284\n",
      "Epoch: 41; Batch 200 / 359; Loss: 0.431376\n",
      "Epoch: 41; Batch 300 / 359; Loss: 0.260110\n",
      "Epoch: 42; Batch 100 / 359; Loss: 0.288444\n",
      "Epoch: 42; Batch 200 / 359; Loss: 0.316924\n",
      "Epoch: 42; Batch 300 / 359; Loss: 0.285830\n",
      "Epoch: 43; Batch 100 / 359; Loss: 0.277388\n",
      "Epoch: 43; Batch 200 / 359; Loss: 0.316379\n",
      "Epoch: 43; Batch 300 / 359; Loss: 0.331850\n",
      "Epoch: 44; Batch 100 / 359; Loss: 0.388178\n",
      "Epoch: 44; Batch 200 / 359; Loss: 0.444902\n",
      "Epoch: 44; Batch 300 / 359; Loss: 0.346832\n",
      "Epoch: 45; Batch 100 / 359; Loss: 0.374450\n",
      "Epoch: 45; Batch 200 / 359; Loss: 0.317200\n",
      "Epoch: 45; Batch 300 / 359; Loss: 0.407524\n",
      "Epoch: 46; Batch 100 / 359; Loss: 0.373883\n",
      "Epoch: 46; Batch 200 / 359; Loss: 0.254787\n",
      "Epoch: 46; Batch 300 / 359; Loss: 0.284466\n",
      "Epoch: 47; Batch 100 / 359; Loss: 0.356887\n",
      "Epoch: 47; Batch 200 / 359; Loss: 0.393670\n",
      "Epoch: 47; Batch 300 / 359; Loss: 0.433978\n",
      "Epoch: 48; Batch 100 / 359; Loss: 0.403218\n",
      "Epoch: 48; Batch 200 / 359; Loss: 0.443052\n",
      "Epoch: 48; Batch 300 / 359; Loss: 0.445759\n",
      "Epoch: 49; Batch 100 / 359; Loss: 0.322762\n",
      "Epoch: 49; Batch 200 / 359; Loss: 0.334234\n",
      "Epoch: 49; Batch 300 / 359; Loss: 0.470618\n",
      "Epoch: 50; Batch 100 / 359; Loss: 0.454097\n",
      "Epoch: 50; Batch 200 / 359; Loss: 0.396877\n",
      "Epoch: 50; Batch 300 / 359; Loss: 0.313683\n",
      "Epoch: 51; Batch 100 / 359; Loss: 0.258424\n",
      "Epoch: 51; Batch 200 / 359; Loss: 0.267575\n",
      "Epoch: 51; Batch 300 / 359; Loss: 0.257421\n",
      "Epoch: 52; Batch 100 / 359; Loss: 0.281594\n",
      "Epoch: 52; Batch 200 / 359; Loss: 0.215101\n",
      "Epoch: 52; Batch 300 / 359; Loss: 0.330670\n",
      "Epoch: 53; Batch 100 / 359; Loss: 0.309614\n",
      "Epoch: 53; Batch 200 / 359; Loss: 0.244599\n",
      "Epoch: 53; Batch 300 / 359; Loss: 0.335611\n",
      "Epoch: 54; Batch 100 / 359; Loss: 0.249704\n",
      "Epoch: 54; Batch 200 / 359; Loss: 0.336069\n",
      "Epoch: 54; Batch 300 / 359; Loss: 0.362213\n",
      "Epoch: 55; Batch 100 / 359; Loss: 0.306742\n",
      "Epoch: 55; Batch 200 / 359; Loss: 0.313078\n",
      "Epoch: 55; Batch 300 / 359; Loss: 0.390532\n",
      "Epoch: 56; Batch 100 / 359; Loss: 0.339597\n",
      "Epoch: 56; Batch 200 / 359; Loss: 0.284014\n",
      "Epoch: 56; Batch 300 / 359; Loss: 0.344179\n",
      "Epoch: 57; Batch 100 / 359; Loss: 0.273187\n",
      "Epoch: 57; Batch 200 / 359; Loss: 0.315988\n",
      "Epoch: 57; Batch 300 / 359; Loss: 0.498405\n",
      "Epoch: 58; Batch 100 / 359; Loss: 0.270276\n",
      "Epoch: 58; Batch 200 / 359; Loss: 0.444988\n",
      "Epoch: 58; Batch 300 / 359; Loss: 0.463111\n",
      "Epoch: 59; Batch 100 / 359; Loss: 0.265708\n",
      "Epoch: 59; Batch 200 / 359; Loss: 0.277454\n",
      "Epoch: 59; Batch 300 / 359; Loss: 0.245659\n",
      "Epoch: 60; Batch 100 / 359; Loss: 0.274673\n",
      "Epoch: 60; Batch 200 / 359; Loss: 0.261088\n",
      "Epoch: 60; Batch 300 / 359; Loss: 0.315197\n",
      "Epoch: 61; Batch 100 / 359; Loss: 0.255041\n",
      "Epoch: 61; Batch 200 / 359; Loss: 0.270385\n",
      "Epoch: 61; Batch 300 / 359; Loss: 0.342473\n",
      "Epoch: 62; Batch 100 / 359; Loss: 0.267052\n",
      "Epoch: 62; Batch 200 / 359; Loss: 0.292082\n",
      "Epoch: 62; Batch 300 / 359; Loss: 0.386246\n",
      "Epoch: 63; Batch 100 / 359; Loss: 0.297236\n",
      "Epoch: 63; Batch 200 / 359; Loss: 0.339367\n",
      "Epoch: 63; Batch 300 / 359; Loss: 0.288527\n",
      "Epoch: 64; Batch 100 / 359; Loss: 0.257321\n",
      "Epoch: 64; Batch 200 / 359; Loss: 0.251663\n",
      "Epoch: 64; Batch 300 / 359; Loss: 0.259630\n",
      "Epoch: 65; Batch 100 / 359; Loss: 0.186305\n",
      "Epoch: 65; Batch 200 / 359; Loss: 0.390512\n",
      "Epoch: 65; Batch 300 / 359; Loss: 0.312033\n",
      "Epoch: 66; Batch 100 / 359; Loss: 0.344832\n",
      "Epoch: 66; Batch 200 / 359; Loss: 0.341699\n",
      "Epoch: 66; Batch 300 / 359; Loss: 0.371183\n",
      "Epoch: 67; Batch 100 / 359; Loss: 0.274668\n",
      "Epoch: 67; Batch 200 / 359; Loss: 0.329647\n",
      "Epoch: 67; Batch 300 / 359; Loss: 0.402587\n",
      "Epoch: 68; Batch 100 / 359; Loss: 0.324443\n",
      "Epoch: 68; Batch 200 / 359; Loss: 0.306198\n",
      "Epoch: 68; Batch 300 / 359; Loss: 0.348775\n",
      "Epoch: 69; Batch 100 / 359; Loss: 0.223155\n",
      "Epoch: 69; Batch 200 / 359; Loss: 0.208625\n",
      "Epoch: 69; Batch 300 / 359; Loss: 0.288433\n",
      "Epoch: 70; Batch 100 / 359; Loss: 0.286312\n",
      "Epoch: 70; Batch 200 / 359; Loss: 0.272747\n",
      "Epoch: 70; Batch 300 / 359; Loss: 0.278193\n",
      "Epoch: 71; Batch 100 / 359; Loss: 0.287306\n",
      "Epoch: 71; Batch 200 / 359; Loss: 0.357541\n",
      "Epoch: 71; Batch 300 / 359; Loss: 0.304520\n",
      "Epoch: 72; Batch 100 / 359; Loss: 0.315393\n",
      "Epoch: 72; Batch 200 / 359; Loss: 0.241246\n",
      "Epoch: 72; Batch 300 / 359; Loss: 0.331347\n",
      "Epoch: 73; Batch 100 / 359; Loss: 0.395164\n",
      "Epoch: 73; Batch 200 / 359; Loss: 0.285743\n",
      "Epoch: 73; Batch 300 / 359; Loss: 0.318700\n",
      "Epoch: 74; Batch 100 / 359; Loss: 0.259040\n",
      "Epoch: 74; Batch 200 / 359; Loss: 0.253852\n",
      "Epoch: 74; Batch 300 / 359; Loss: 0.258350\n",
      "Epoch: 75; Batch 100 / 359; Loss: 0.209035\n",
      "Epoch: 75; Batch 200 / 359; Loss: 0.161726\n",
      "Epoch: 75; Batch 300 / 359; Loss: 0.214476\n",
      "Epoch: 76; Batch 100 / 359; Loss: 0.239874\n",
      "Epoch: 76; Batch 200 / 359; Loss: 0.362325\n",
      "Epoch: 76; Batch 300 / 359; Loss: 0.409650\n",
      "Epoch: 77; Batch 100 / 359; Loss: 0.283337\n",
      "Epoch: 77; Batch 200 / 359; Loss: 0.269133\n",
      "Epoch: 77; Batch 300 / 359; Loss: 0.328553\n",
      "Epoch: 78; Batch 100 / 359; Loss: 0.187453\n",
      "Epoch: 78; Batch 200 / 359; Loss: 0.346907\n",
      "Epoch: 78; Batch 300 / 359; Loss: 0.305480\n",
      "Epoch: 79; Batch 100 / 359; Loss: 0.244893\n",
      "Epoch: 79; Batch 200 / 359; Loss: 0.331215\n",
      "Epoch: 79; Batch 300 / 359; Loss: 0.354589\n",
      "Epoch: 80; Batch 100 / 359; Loss: 0.260977\n",
      "Epoch: 80; Batch 200 / 359; Loss: 0.300644\n",
      "Epoch: 80; Batch 300 / 359; Loss: 0.269115\n",
      "Epoch: 81; Batch 100 / 359; Loss: 0.277857\n",
      "Epoch: 81; Batch 200 / 359; Loss: 0.282777\n",
      "Epoch: 81; Batch 300 / 359; Loss: 0.241357\n",
      "Epoch: 82; Batch 100 / 359; Loss: 0.292800\n",
      "Epoch: 82; Batch 200 / 359; Loss: 0.258139\n",
      "Epoch: 82; Batch 300 / 359; Loss: 0.218701\n",
      "Epoch: 83; Batch 100 / 359; Loss: 0.226314\n",
      "Epoch: 83; Batch 200 / 359; Loss: 0.272486\n",
      "Epoch: 83; Batch 300 / 359; Loss: 0.383008\n",
      "Epoch: 84; Batch 100 / 359; Loss: 0.221763\n",
      "Epoch: 84; Batch 200 / 359; Loss: 0.327920\n",
      "Epoch: 84; Batch 300 / 359; Loss: 0.251757\n",
      "Epoch: 85; Batch 100 / 359; Loss: 0.381044\n",
      "Epoch: 85; Batch 200 / 359; Loss: 0.259201\n",
      "Epoch: 85; Batch 300 / 359; Loss: 0.377330\n",
      "Epoch: 86; Batch 100 / 359; Loss: 0.230331\n",
      "Epoch: 86; Batch 200 / 359; Loss: 0.261134\n",
      "Epoch: 86; Batch 300 / 359; Loss: 0.216988\n",
      "Epoch: 87; Batch 100 / 359; Loss: 0.204717\n",
      "Epoch: 87; Batch 200 / 359; Loss: 0.399959\n",
      "Epoch: 87; Batch 300 / 359; Loss: 0.330260\n",
      "Epoch: 88; Batch 100 / 359; Loss: 0.259635\n",
      "Epoch: 88; Batch 200 / 359; Loss: 0.249721\n",
      "Epoch: 88; Batch 300 / 359; Loss: 0.400023\n",
      "Epoch: 89; Batch 100 / 359; Loss: 0.193958\n",
      "Epoch: 89; Batch 200 / 359; Loss: 0.253830\n",
      "Epoch: 89; Batch 300 / 359; Loss: 0.258956\n",
      "Epoch: 90; Batch 100 / 359; Loss: 0.214421\n",
      "Epoch: 90; Batch 200 / 359; Loss: 0.311594\n",
      "Epoch: 90; Batch 300 / 359; Loss: 0.220369\n",
      "Epoch: 91; Batch 100 / 359; Loss: 0.362918\n",
      "Epoch: 91; Batch 200 / 359; Loss: 0.230590\n",
      "Epoch: 91; Batch 300 / 359; Loss: 0.225697\n",
      "Epoch: 92; Batch 100 / 359; Loss: 0.195757\n",
      "Epoch: 92; Batch 200 / 359; Loss: 0.244210\n",
      "Epoch: 92; Batch 300 / 359; Loss: 0.367357\n",
      "Epoch: 93; Batch 100 / 359; Loss: 0.220971\n",
      "Epoch: 93; Batch 200 / 359; Loss: 0.245221\n",
      "Epoch: 93; Batch 300 / 359; Loss: 0.554574\n",
      "Epoch: 94; Batch 100 / 359; Loss: 0.426596\n",
      "Epoch: 94; Batch 200 / 359; Loss: 0.425205\n",
      "Epoch: 94; Batch 300 / 359; Loss: 0.240387\n",
      "Epoch: 95; Batch 100 / 359; Loss: 0.229236\n",
      "Epoch: 95; Batch 200 / 359; Loss: 0.212456\n",
      "Epoch: 95; Batch 300 / 359; Loss: 0.169494\n",
      "Epoch: 96; Batch 100 / 359; Loss: 0.191481\n",
      "Epoch: 96; Batch 200 / 359; Loss: 0.276937\n",
      "Epoch: 96; Batch 300 / 359; Loss: 0.224435\n",
      "Epoch: 97; Batch 100 / 359; Loss: 0.292725\n",
      "Epoch: 97; Batch 200 / 359; Loss: 0.293787\n",
      "Epoch: 97; Batch 300 / 359; Loss: 0.246286\n",
      "Epoch: 98; Batch 100 / 359; Loss: 0.190682\n",
      "Epoch: 98; Batch 200 / 359; Loss: 0.271781\n",
      "Epoch: 98; Batch 300 / 359; Loss: 0.278178\n",
      "Epoch: 99; Batch 100 / 359; Loss: 0.252225\n",
      "Epoch: 99; Batch 200 / 359; Loss: 0.261893\n",
      "Epoch: 99; Batch 300 / 359; Loss: 0.243092\n",
      "Epoch: 100; Batch 100 / 359; Loss: 0.226614\n",
      "Epoch: 100; Batch 200 / 359; Loss: 0.213135\n",
      "Epoch: 100; Batch 300 / 359; Loss: 0.241233\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m225\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(num_epochs, model, train_dataloader, loss_func, optimizer)\u001b[0m\n\u001b[0;32m     12\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train(num_epochs=225, model=model, loss_func=loss_function, optimizer=optimizer, train_dataloader=dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def create_network(network_input, n_vocab):\n",
    "#     \"\"\" create the structure of the neural network \"\"\"\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(\n",
    "#         512,\n",
    "#         input_shape=(network_input.shape[1], network_input.shape[2]), #input_shape=(# notes in a sequence = 100, # notes at once = 1)\n",
    "#         recurrent_dropout=0.3,\n",
    "#         return_sequences=True\n",
    "#     ))\n",
    "#     model.add(LSTM(512, return_sequences=True, recurrent_dropout=0.3,))\n",
    "#     model.add(LSTM(512))\n",
    "#     model.add(BatchNorm())\n",
    "#     model.add(Dropout(0.3))\n",
    "#     model.add(Dense(256))\n",
    "#     model.add(Activation('relu'))\n",
    "#     model.add(BatchNorm())\n",
    "#     model.add(Dropout(0.3))\n",
    "#     model.add(Dense(n_vocab))\n",
    "#     model.add(Activation('softmax'))\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "#     return model\n",
    "\n",
    "# def train(model, network_input, network_output):\n",
    "#     \"\"\" train the neural network \"\"\"\n",
    "#     filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "#     checkpoint = ModelCheckpoint(\n",
    "#         filepath,\n",
    "#         monitor='loss',\n",
    "#         verbose=0,\n",
    "#         save_best_only=True,\n",
    "#         mode='min'\n",
    "#     )\n",
    "#     callbacks_list = [checkpoint]\n",
    "\n",
    "#     model.fit(network_input, network_output, epochs=200, batch_size=128, callbacks=callbacks_list)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     train_network()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
